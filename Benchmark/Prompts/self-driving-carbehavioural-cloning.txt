DATASET METADATA:

{
  "id": "aslanahmedov/self-driving-carbehavioural-cloning",
  "id_no": 1807203,
  "datasetSlugNullable": "self-driving-carbehavioural-cloning",
  "ownerUserNullable": "aslanahmedov",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Self Driving Car",
  "subtitleNullable": "Behavioural Cloning Complete Guide",
  "descriptionNullable": "![ezgif com-gif-maker](https://user-images.githubusercontent.com/91852182/147305077-8b86ec92-ed26-43ca-860c-5812fea9b1d8.gif)\n\n# SELF-DRIVING CAR USING UDACITY\u2019S CAR SIMULATOR ENVIRONMENT AND TRAINED BY DEEP NEURAL NETWORKS COMPLETE GUIDE\n\n\n## Table of Contents\n### Introduction\n\n- Problem Definition\n- Solution Approach\n- Technologies Used\n- Convolutional Neural Networks (CNN)\n- Time-Distributed Layers\n\n### Udacity Simulator and Dataset\n### The Training Process\n### Augmentation and image pre-processing\n### Experimental configurations\n### Network architectures\n### Results\n\n- Value loss or Accuracy\n- Why We Use ELU Over RELU\n\n\n### The Connection Part\n### Files\n### Overview\n### References\n\n## Introduction \nSelf-drivi cars has become a trending subject with a significant improvement in the technologies in the last decade. The project purpose is to train a neural network to drive an autonomous car agent on the tracks of Udacity\u2019s Car Simulator environment. Udacity has released the simulator as an open source software and  enthusiasts have hosted a competition (challenge) to teach a car how to drive using only camera images and deep learning. Driving a car in an autonomous manner requires learning to control steering angle, throttle and brakes. Behavioral cloning technique is used to mimic human driving behavior in the training mode on the track. That means a dataset is generated in the simulator by user driven car in training mode, and the deep neural network model then drives the car in autonomous mode. Ultimately, the car was able to run on Track 1 generalizing well. The project aims at reaching the same accuracy on real time data in the future.![6](https://user-images.githubusercontent.com/91852182/147298831-225740f9-6903-4570-8336-0c9f16676456.png)\n\n\n### Problem Definition\n\nUdacity released an open source simulator for self-driving cars to depict a real-time environment. The challenge is to mimic the driving behavior of a human on the simulator with the help of a model trained by deep neural networks. The concept is called Behavioral Cloning, to mimic how a human drives. The simulator contains two tracks and two modes, namely, training mode and autonomous mode. The dataset is generated from the simulator by the user, driving the car in training mode. This dataset is also known as the \u201cgood\u201d driving data. This is followed by testing on the track, seeing how the deep learning model performs after being trained by that user data.\n\n### Solution Approach\n ![1](https://user-images.githubusercontent.com/91852182/147298261-4d57a5c1-1fda-4654-9741-2f284e6d0479.png)\n \n The problem is solved in the following steps: \n \n- The simulator can be used to collect data by driving the car in the training mode using a joystick or keyboard, providing the so called \u201cgood-driving\u201d behavior input data in form of a driving_log (.csv file) and a set of images. The simulator acts as a server and pipes these images and data log to the Python client. \n- The client (Python program) is the machine learning model built using Deep Neural Networks. These models are developed on Keras (a high-level API over Tensorflow). Keras provides sequential models to build a linear stack of network layers. Such models are used in the project to train over the datasets as the second step. Detailed description of CNN models experimented and used can be referred to in the chapter on network architectures. \n- Once the model is trained, it provides steering angles and throttle to drive in an autonomous mode to the server (simulator). \n- These modules, or inputs, are piped back to the server and are used to drive the car autonomously in the simulator and keep it from falling off the track.\n\n### Technologies Used \n\nTechnologies that are used in the implementation of this project and the motivation behind using these are described in this section.\n \nTensorFlow: This an open-source library for dataflow programming. It is widely used for machine learning applications. It is also used as both a math library and for large computation. For this project Keras, a high-level API that uses TensorFlow as the backend is used. Keras facilitate in building the models easily as it more user friendly. \n\nDifferent libraries are available in Python that helps in machine learning projects. Several of those libraries have improved the performance of this project. Few of them are mentioned in this section. First, \u201cNumpy\u201d that provides with high-level math function collection to support multi-dimensional metrices and arrays. This is used for faster computations over the weights (gradients) in neural networks. Second, \u201cscikit-learn\u201d is a machine learning library for Python which features different algorithms and Machine Learning function packages. Another one is OpenCV (Open Source Computer Vision Library) which is designed for computational efficiency with focus on real-time applications. In this project, OpenCV is used for image preprocessing and augmentation techniques. \n\nThe project makes use of Conda Environment which is an open source distribution for Python which simplifies package management and deployment. It is best for large scale data processing. The machine on which this project was built, is a personal computer. \n\n### Convolutional Neural Networks (CNN)\n\nCNN is a type of feed-forward neural network computing system that can be used to learn from input data. Learning is accomplished by determining a set of weights or filter values that allow the network to model the behavior according to the training data. The desired output and the output generated by CNN initialized with random weights will be different. This difference (generated error) is backpropagated through the layers of CNN to adjust the weights of the neurons, which in turn reduces the error and allows us produce output closer to the desired one. \n\nCNN is good at capturing hierarchical and spatial data from images. It utilizes filters that look at regions of an input image with a defined window size and map it to some output. It then slides the window by some defined stride to other regions, covering the whole image. Each convolution filter layer thus captures the properties of this input image hierarchically in a series of subsequent layers, capturing the details like lines in image, then shapes, then whole objects in later layers. CNN can be a good fit to feed the images of a dataset and classify them into their respective classes. \n\n### Time-Distributed Layers\n\nAnother type of layers sometimes used in deep learning networks is a Time- distributed layer. Time-Distributed layers are provided in Keras as wrapper layers. Every temporal slice of an input is applied with this wrapper layer. The requirement for input is that to be at least three-dimensional, first index can be considered as temporal dimension. These Time-Distributed can be applied to a dense layer to each of the timesteps, independently or even used with Convolutional Layers. The way they can be written is also simple in Keras as shown in Figure 1 and Figure 2.\n\n![2](https://user-images.githubusercontent.com/91852182/147298483-4f37a092-7e71-4ce6-9274-9a133d138a4c.png)\n\nFig. 1: TimeDistributed Dense layer\n\n![3](https://user-images.githubusercontent.com/91852182/147298501-6459d968-a279-4140-9be3-2d3ea826d9f6.png)\n\nFig. 2: TimeDistributed Convolution layer\n\n## Udacity Simulator and Dataset \n\nWe will first download the [simulator](https://github.com/udacity/self-driving-car-sim) to start our behavioural training process. Udacity has built a simulator for self-driving cars and made it open source for the enthusiasts, so they can work on something close to a real-time environment. It is built on Unity, the video game development platform. The simulator consists of a configurable resolution and controls setting and is very user friendly. The graphics and input configurations can be changed according to user preference and machine configuration as shown in Figure 3. The user pushes the \u201cPlay!\u201d button to enter the simulator user interface. You can enter the Controls tab to explore the keyboard controls, quite similar to a racing game which can be seen in Figure 4. \n\n![ 4](https://user-images.githubusercontent.com/91852182/147298708-de15ebc5-2482-42f8-b2a2-8d3c59fceff4.png)\n\nFig. 3: Configuration screen                                                                    \n\n![5](https://user-images.githubusercontent.com/91852182/147298712-944e2c2d-e01d-459b-8a7d-3c5471bea179.png)\n\nFig. 4: Controls Configuration\n\nThe first actual screen of the simulator can be seen in Figure 5 and its components are discussed below. The simulator involves two tracks. One of them can be considered as simple and another one as complex that can be evident in the screenshots attached in Figure 6 and Figure 7. The word \u201csimple\u201d here just means that it has fewer curvy tracks and is easier to drive on, refer Figure 6. The \u201ccomplex\u201d track has steep elevations, sharp turns, shadowed environment, and is tough to drive on, even by a user doing it manually. Please refer Figure 6. There are two modes for driving the car in the simulator: (1) Training mode and (2) Autonomous mode. The training mode gives you the option of recording your run and capturing the training dataset. The small red sign at the top right of the screen in the Figure 6 and 7 depicts the car is being driven in training mode. The autonomous mode can be used to test the models to see if it can drive on the track without human intervention. Also, if you try to press the controls to get the car back on track, it will immediately notify you that it shifted to manual controls. The mode screenshot can be as seen in Figure 8. Once we have mastered how the car driven controls in simulator using keyboard keys, then we get started with record button to collect data. We will save the data from it in a specified folder as you can see below.\n\n![6](https://user-images.githubusercontent.com/91852182/147298837-17eecb80-0a3f-4edb-a5f3-050a318f66e0.png)\n\n<img alt=\"7\" src=\"https://user-images.githubusercontent.com/91852182/147298975-e05dc738-2fb7-4dca-a28d-9756285d94cc.png\">\n\nThe simulator\u2019s feature to create your own dataset of images makes it easy to work on the problem. Some reasons why this feature is useful are as follows: \n\n- The simulator has built the driving features in such a way that it simulates that there are three cameras on the car. The three cameras are in the center, right and left on the front of the car, which captures continuously when we record in the training mode. \n- The stream of images is captured, and we can set the location on the disk for saving the data after pushing the record button. The image set are labelled in a sophisticated manner with a prefix of center, left, or right indicating from which camera the image has been captured. \n- Along with the image dataset, it also generates a datalog.csv file. This file contains the image paths with corresponding steering angle, throttle, brakes, and speed of the car at that instance. \n\nA few images from the dataset are shown below .\n\n<img alt=\"8\" src=\"https://user-images.githubusercontent.com/91852182/147299066-17bb3db0-5f1c-44ff-ab7e-41786c243d4f.png\">\n\n# Pic wich i have in folder IMG only for visualization your data will be more than 5000 pic.\n\nA sample of driving_log.csv file is shown in Figure 9.\n\n* *Column 1, 2, 3:*  contains paths to the dataset images of center, right and left respectively Column 4: contains the steering angle \n* *Column 4:*  value as 0 depicts straight, positive value is right turn and negative value is left turn. \n* *Column 5:*  contains the throttle or acceleration at that instance \n* *Column 6:*  contains the brakes or deceleration at that instance \n* *Column 7:*  contains the speed of the vehicle \n\n![9](https://user-images.githubusercontent.com/91852182/147299491-77d853c4-604b-42ef-8db4-2469a2993e00.png)\n\nFig 9 . driving_log.csv\n\n## The Training Process\n\nFor the process of getting the self driving car working, we have to upload the images that we recorded using the simulator. First of all, we will open [GitHub Desktop.](https://desktop.github.com/)If we do not have an account, we will create a new one. With that, we will create a new repository.\n\n![10](https://user-images.githubusercontent.com/91852182/147299586-a4afa457-d6ed-4383-a8d9-adf6fbcded47.png)\n\nWe will be using [Google Colab](https://colab.research.google.com/) for doing the training process or [Kaggle](https://www.kaggle.com/).\nWe will open a new python3 notebook and get started. Next, we will git clone the repo.\n\n```!git clone https://github.com/Asikpalysik/Self-Driving-Car.git```\n \nWe will now import all the libraries needed for training process. It will use Tensorflow backend and keras at frontend.\n \n```\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom imgaug import augmenters as iaa\nimport cv2\nimport pandas as pd\nimport ntpath\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\nWe wil use datadir as the name given to the folder itself and take the parameters itself. Using head, we will show the first five values for the CSV on the desired format\n\n```\ndir = \"/Users/asik/Desktop/SelfDrivingCar\"\ncolumns = [\"center\", \"left\", \"right\", \"steering\", \"throttle\", \"reverse\", \"speed\"]\ndata = pd.read_csv(os.path.join(dir, \"driving_log.csv\"), names=columns)\npd.set_option(\"display.max_colwidth\", -1)\ndata.head()\n```\n\n![11](https://user-images.githubusercontent.com/91852182/147300673-09faac02-c98a-4139-9151-b5da96b74593.png)\n\nAs this is picking up the entire path from the local machine, we need to use ntpath function to get the network path assigned. We will declare a name path_leaf and assign accordingly.\n\n```\ndef pathleaf(path):\n    head, tail = ntpath.split(path)\n    return tail\n    \ndata[\"center\"] = data[\"center\"].apply(pathleaf)\ndata[\"left\"] = data[\"left\"].apply(pathleaf)\ndata[\"right\"] = data[\"right\"].apply(pathleaf)\ndata.head()\n```\n\n![12](https://user-images.githubusercontent.com/91852182/147300702-81a81f27-41f8-49a6-843b-67838b1bcb84.png)\n\nWe will bin the number of values where the number will be equal to 25 (odd number aimed to get center distribution). We will see the histogram using the np.histogram option on data frame \u2018steering\u2019, we will divide it to the number of bins. We keep samples at 400 and then we draw a line. We see the data is centered along the middle that is 0.\n\n```\nnum_bins = 25\nsamples_per_bin = 400\nhist, bins = np.histogram(data[\"steering\"], num_bins)\nprint(bins)\n```\n\n![13](https://user-images.githubusercontent.com/91852182/147300730-c543c7cd-9ced-4019-9aa8-555e44b123cc.png)\n\nPlot on it\n\n```\ncenter = (bins[:-1] + bins[1:]) * 0.5\nplt.bar(center, hist, width=0.05)\nplt.plot(\n    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n    (samples_per_bin, samples_per_bin),\n)\n```\n\n<img alt=\"14\" src=\"https://user-images.githubusercontent.com/91852182/147301498-cb9aac07-837e-4f66-8825-36608843dab4.png\">\n\n```\nprint(\"Total Data:\", len(data))\n```\n-&gt;&gt;Total Data: 1795\n\n\nWe wil specify a variable remove_list.We will specify samples we want to remove using looping construct through every single bin we will iterate through all the steering data. We will shuffle the data and romve some from it as it is now uniformly structured after shuffling.The output will be the distribution of steering angle that are much more uniform. There are significant amount of left steering angle and right steering angle eliminating the bias to drive straight all the time.\n\n```\nremove_list = []\nfor j in range(num_bins):\n    list_ = []\n    for i in range(len(data[\"steering\"])):\n        if data[\"steering\"][i] &gt;= bins[j] and data[\"steering\"][i] &lt;= bins[j + 1]:\n            list_.append(i)\n    list_ = shuffle(list_)\n    list_ = list_[samples_per_bin:]\n    remove_list.extend(list_)\nprint(\"Removed:\", len(remove_list))\n```\n-&gt;&gt; Removed: 945\n\n```\ndata.drop(data.index[remove_list], inplace=True)\nprint(\"Remaining:\", len(data))\n-&gt;&gt;Remaining: 850\n```\n\nPlot on it\n\n```\nhist, _ = np.histogram(data[\"steering\"], (num_bins))\nplt.bar(center, hist, width=0.05)\nplt.plot(\n    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n    (samples_per_bin, samples_per_bin),\n)\n```\n\n<img alt=\"15\" src=\"https://user-images.githubusercontent.com/91852182/147301598-185b5e70-17f8-41db-81d3-50cfc5db3902.png\">\n\n```\nprint(data.iloc[1])\n```\n\n![16](https://user-images.githubusercontent.com/91852182/147301621-fcfb3f40-607f-4216-af9e-d82967e24bc9.png)\n\nWe will now load the image into array to manipulate them accordingly. We will define a function named locd_img_steering. We will have image path as empty list and steering as empty list and then loop through. We use iloc selector as data frame based on the specific index, we will use cut data for now.\n\n```\ndef load_img_steering(datadir, df):\n    image_path = []\n    steering = []\n    for i in range(len(data)):\n        indexed_data = data.iloc[i]\n        center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]\n        image_path.append(os.path.join(datadir, center.strip()))\n        steering.append(float(indexed_data[3]))\n        image_path.append(os.path.join(datadir, left.strip()))\n        steering.append(float(indexed_data[3]) + 0.15)\n        image_path.append(os.path.join(datadir, right.strip()))\n        steering.append(float(indexed_data[3]) - 0.15)\n    image_paths = np.asarray(image_path)\n    steerings = np.asarray(steering)\n    return image_paths, steerings\n```\n\nWe will be splitting the image path as well as storing arrays accordingly.\n\n```\nimage_paths, steerings = load_img_steering(dir + \"/IMG\", data)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    image_paths, steerings, test_size=0.2, random_state=6\n)\nprint(\"Training Samples: {}\\nValid Samples: {}\".format(len(X_train), len(X_valid)))\n```\n-&gt;&gt;Training Samples: 2040 \n\n-&gt;&gt;Valid Samples: 510\n\nWe will have the histograms now.\n\n```\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].hist(y_train, bins=num_bins, width=0.05, color=\"blue\")\naxes[0].set_title(\"Training set\")\naxes[1].hist(y_valid, bins=num_bins, width=0.05, color=\"red\")\naxes[1].set_title(\"Validation set\")\n```\n\n<img alt=\"17\" src=\"https://user-images.githubusercontent.com/91852182/147301799-0e3ca6a5-6f4e-41a9-b2c5-7bf5b1dad400.png\">\n\n## Augmentation and image pre-processing\nThe biggest challenge was generalizing the behavior of the car on Track_2 which it was never trained for. In a real-life situation, we can never train a self-driving car model for every track possible, as the data will be too huge to process. Also, it is not possible to gather the dataset for all the weather conditions and roads. Thus, there is a need to come up with an idea of generalizing the behavior on different tracks. This problem is solved using image preprocessing and augmentation techniques.\n\n- Zoom \n\n\nThe images in the dataset have relevant features in the lower part where the road is visible. The external environment above a certain image portion will never be used to determine the output and thus can be cropped. Approximately, 30% of the top portion of the image is cut and passed in the training set. The snippet of code and transformation of an image after cropping and resizing it to original image can be seen in  below.\n\n<img alt=\"18\" src=\"https://user-images.githubusercontent.com/91852182/147301992-fdc9d029-3a33-4f2f-8dc2-222067a07a0a.png\">\n\n```\ndef zoom(image):\n    zoom = iaa.Affine(scale=(1, 1.3))\n    image = zoom.augment_image(image)\n    return image\n\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\nzoomed_image = zoom(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(zoomed_image)\naxs[1].set_title(\"Zoomed Image\")\n```\n\n- Flip (horizontal) \n\nThe image is flipped horizontally (i.e. a mirror image of the original image is passed to the dataset). The motive behind this is that the model gets trained for similar kinds of turns on opposite sides too. This is important because Track 1 includes only left turns. The snippet of code and transformation of an image after flipping it can be seen in below. \n\n<img alt=\"19\" src=\"https://user-images.githubusercontent.com/91852182/147302076-3bd4311e-79b2-4750-84a7-c1061ffcdae3.png\">\n\n```\ndef random_flip(image, steering_angle):\n    image = cv2.flip(image, 1)\n    steering_angle = -steering_angle\n    return image, steering_angle\n\nrandom_index = random.randint(0, 1000)\nimage = image_paths[random_index]\nsteering_angle = steerings[random_index]\n\noriginal_image = mpimg.imread(image)\nflipped_image, flipped_steering_angle = random_flip(original_image, steering_angle)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image - \" + \"Steering Angle:\" + str(steering_angle))\n\naxs[1].imshow(flipped_image)\naxs[1].set_title(\"Flipped Image - \" + \"Steering Angle:\" + str(flipped_steering_angle))\n```\n\n- Shift (horizontal/vertical)\n\n\nThe image is shifted by a small amount, it is vertical shift and horizontal shift as below.\n\n\n<img alt=\"20\" src=\"https://user-images.githubusercontent.com/91852182/147302200-34bae3a8-2236-42d7-819d-ea694a382a02.png\">\n\n```\ndef pan(image):\n    pan = iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n    image = pan.augment_image(image)\n    return image\n\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\npanned_image = pan(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(panned_image)\naxs[1].set_title(\"Panned Image\")\n```\n\n- Brightness \n\n\nTo generalize to the weather conditions with bright sunny day or cloudy, lowlight conditions, the brightness augmentation can prove to be very useful. The code snippet and increase of brightness can be seen below. Similarly, I have randomly also lowered down the level of brightness for other conditions. \n\n<img alt=\"21\" src=\"https://user-images.githubusercontent.com/91852182/147302277-defccf7a-43f4-459c-a0db-536f01b77110.png\">\n\n```\ndef random_brightness(image):\n    brightness = iaa.Multiply((0.2, 1.2))\n    image = brightness.augment_image(image)\n    return image\n\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\nbrightness_altered_image = random_brightness(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(brightness_altered_image)\naxs[1].set_title(\"Brightness altered image \")\n```\n\nTo have a look what we have at this moment\n\n```\ndef random_augment(image, steering_angle):\n    image = mpimg.imread(image)\n    if np.random.rand() &lt; 0.5:\n        image = pan(image)\n    if np.random.rand() &lt; 0.5:\n        image = zoom(image)\n    if np.random.rand() &lt; 0.5:\n        image = random_brightness(image)\n    if np.random.rand() &lt; 0.5:\n        image, steering_angle = random_flip(image, steering_angle)\n    return image, steering_angle\n\nncol = 2\nnrow = 10\n\nfig, axs = plt.subplots(nrow, ncol, figsize=(15, 50))\nfig.tight_layout()\n\nfor i in range(10):\n    randnum = random.randint(0, len(image_paths) - 1)\n    random_image = image_paths[randnum]\n    random_steering = steerings[randnum]\n    original_image = mpimg.imread(random_image)\n    augmented_image, steering = random_augment(random_image, random_steering)\n    axs[i][0].imshow(original_image)\n    axs[i][0].set_title(\"Original Image\")\n\n    axs[i][1].imshow(augmented_image)\n    axs[i][1].set_title(\"Augmented Image\")\n```\n\n![23](https://user-images.githubusercontent.com/91852182/147302403-33b0de7e-5cab-46b1-a16a-77321cec9cbe.png)\n![24](https://user-images.githubusercontent.com/91852182/147302408-aadfd20d-6cc0-43cf-9da7-39e1ada3fe9a.png)\n\n\nI continued by doing some image processing. I cropped the image to remove the unnecessary features, changes the images to YUV format, used gaussian blur, decreased the size for easier processing and normalized the values.\n\n```\ndef img_preprocess(img):\n    ## Crop image to remove unnecessary features\n    img = img[60:135, :, :]\n    ## Change to YUV image\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n    ## Gaussian blur\n    img = cv2.GaussianBlur(img, (3, 3), 0)\n    ## Decrease size for easier processing\n    img = cv2.resize(img, (200, 66))\n    ## Normalize values\n    img = img / 255\n    return img\n```\n\nTo compare and visualize I plotted the original and the pre-processed image.\n\n```\nimage = image_paths[100]\noriginal_image = mpimg.imread(image)\npreprocessed_image = img_preprocess(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(preprocessed_image)\naxs[1].set_title(\"Preprocessed Image\")\n```\n\n<img alt=\"25\" src=\"https://user-images.githubusercontent.com/91852182/147302472-ac64e7d7-1ec5-403a-82ff-046258584145.png\">\n\n```\ndef batch_generator(image_paths, steering_ang, batch_size, istraining):\n    while True:\n        batch_img = []\n        batch_steering = []\n\n        for i in range(batch_size):\n            random_index = random.randint(0, len(image_paths) - 1)\n\n            if istraining:\n                im, steering = random_augment(\n                    image_paths[random_index], steering_ang[random_index]\n                )\n\n            else:\n                im = mpimg.imread(image_paths[random_index])\n                steering = steering_ang[random_index]\n\n            im = img_preprocess(im)\n            batch_img.append(im)\n            batch_steering.append(steering)\n\n        yield (np.asarray(batch_img), np.asarray(batch_steering))\n```\n\nSo far so good. Next, I converted all the images into numpy array.\n\n```\nx_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))\nx_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(x_train_gen[0])\naxs[0].set_title(\"Training Image\")\n\naxs[1].imshow(x_valid_gen[0])\naxs[1].set_title(\"Validation Image\")\n```\n\n<img alt=\"26\" src=\"https://user-images.githubusercontent.com/91852182/147302514-54b40af4-baea-491e-ba7b-73e6d752872e.png\">\n\n## Experimental configurations\n\nConfigurations used to set up the models for training the Python Client to provide the Neural Network outputs that drive the car on the simulator. The tweaking of parameters and rigorous experiments were tried to reach the best combination. Though each of the models had their unique behaviors and differed in their performance with each tweak, the following combination of configuration can be considered as the optimal: \n\n- The sequential models built on Keras with deep neural network layers are used to train the data. \n- Models are only trained using the dataset from Track 1. \n- 80% of the dataset is used for training, 20% is used for testing. \n- Epochs = 10, i.e. number of iterations or passes through the complete dataset. Experimented with larger number of epochs also, but the model tried to \u201coverfit\u201d. In other words, the model learns the details in the training data too well, while impacting the performance on new dataset. \n- Batch-size = 100, i.e. number of image samples propagated through the network, like a subset of data as complete dataset is too big to be passed all at once.\n- Learning rate = 0.0001, i.e. how the coefficients of the weights or gradients change in the network. \n\nThere are different combinations of Convolution layer, Time-Distributed layer, MaxPooling layer, Flatten, Dropout, Dense and so on, that can be used to implement the Neural Network models. \n\n## Network architectures\n\nThe design of the network is based on the NVIDIA model, which has been used by NVIDIA for the end-to-end self driving test. As such, it is well suited for the project.It is a deep convolution network which works well with supervised image classification / regression problems. As the NVIDIA model is well documented, I was able to focus how to adjust the training images to produce the best result with some adjustments to the model to avoid overfitting and adding non-linearity to improve the prediction.\n\nI've added the following adjustments to the model.\n\n- I used Lambda layer to normalized input images to avoid saturation and make gradients work better.\n- I've added an additional dropout layer to avoid overfitting after the convolution layers.\n- I've also included ELU for activation function for every layer except for the output layer to introduce non-linearity.\n\nIn the end, the model looks like as follows:\n\n- Image normalization\n- Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n- Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n- Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n- Drop out (0.5)\n- Fully connected: neurons: 100, activation: ELU\n- Fully connected: neurons: 50, activation: ELU\n- Fully connected: neurons: 10, activation: ELU\n- Fully connected: neurons: 1 (output)\n\n![27](https://user-images.githubusercontent.com/91852182/147302681-661ef73d-8cb1-452f-bbcd-323052972189.png)\n\nAs per the NVIDIA model, the convolution layers are meant to handle feature engineering and the fully connected layer for predicting the steering angle. However, as stated in the NVIDIA document, it is not clear where to draw such a clear distinction. Overall, the model is very functional to clone the given steering behavior. The below is a model structure output from the Keras which gives more details on the shapes and the number of parameters.\n\nWe will design our Model architecture. We have to classify the traffic signs too that's why we need to shift from Lenet 5 model to NVDIA model. With behavioural cloning, our dataset is much more complex then any dataset we have used. We are dealing with images that have (200,66) dimensions. Our current datset has 5386 images to train with but MNSIT has around 60,000 images to train with. Our behavioural cloning code has simply has to return appropriate steering angle which is a regression type example. For these things, we need a more advanced model which is provided by nvdia and known as nvdia model.\n\nFor defining the model architecture, we need to define the model object. Normalization state can be skipped as we have already normalized it. We will add the convolution layer. As compared to the model, we will organize accordingly. The Nvdia model uses 24 filters in the layer along with a kernel of size 5,5. We will introduce sub sampling. The function reflects to stride length of the kernel as it processes through an image, we have large images. Horizontal movement with 2 pixels at a time, similarly vertical movement to 2 pixels at a time. As this is the first layer, we have to define input shape of the model too i.e., (66,200,3) and the last function is an activation function that is \u201celu\u201d.\n\nRevisting the model, we see that our second layer has 36 filters with kernel size (5,5) same subsampling option with stride length of (2,2) and conclude this layer with activation \u2018elu\u2019.\n\nAccording to Nvdia model, it shows we have 3 more layers in the convolutional neural network. With 48 filters, with 64 filters (3,3) kernel 64 filters (3,3) kernel Dimensions have been reduced significantly so for that we will remove subsampling from 4th and 5th layer.\n\nNext we add a flatten layer. We will take the output array from previous convolution neural network to convert it into a one dimensional array so that it can be fed to fully connected layer to follow.\n\nOur last convolution layer outputs an array shape of (1,18) by 64.\n\nWe end the architecture of Nvdia model with a dense layer containing a single output node which will output the predicted steering angle for our self driving car. Now we will use model.compile() to compile our architecture as this is a regression type example the metrics that we will be using will be mean squared error and optimize as Adam. We will be using relatively a low learning rate that it can help on accuracy. We will use dropout layer to avoid overfitting the data. Dropout Layer sets the input of random fraction of nodes to \u201c0\u201d during each update. During this, we will generate the training data as it is forced to use a variety of combination of nodes to learn from the same data. We will have to separate the convolution layer from fully connected layer with a factor of 0.5 is added so it converts 50 percent of the input to 0. We Will define the model by calling the nvdia model itself. Now we will have the model training process.To define training parameters, we will use model.fit(), we will import our training data X_Train, training data -&gt;y_train, we have less data on the datasets we will require more epochs to be effective. We will use validation data and then use Batch size.\n\n\n```\ndef NvidiaModel():\n  model = Sequential()\n  model.add(Convolution2D(24,(5,5),strides=(2,2),input_shape=(66,200,3),activation=\"elu\"))\n  model.add(Convolution2D(36,(5,5),strides=(2,2),activation=\"elu\"))\n  model.add(Convolution2D(48,(5,5),strides=(2,2),activation=\"elu\")) \n  model.add(Convolution2D(64,(3,3),activation=\"elu\"))   \n  model.add(Convolution2D(64,(3,3),activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Flatten())\n  model.add(Dense(100,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Dense(50,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Dense(10,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Dense(1))\n  model.compile(optimizer=Adam(lr=1e-3),loss=\"mse\")\n  return model\n```\n\n```\nmodel = NvidiaModel()\nprint(model.summary())\n```\n\n![28](https://user-images.githubusercontent.com/91852182/147302750-f96a9310-e1d7-475d-bb55-d02c457d2139.png)\n\n## Results\n\nThe following results were observed for described architectures. I had to come up with two different performance metrics. \n- Value loss or Accuracy (computed during training phase) \n- Generalization on Track 1 (drive performance)\n\n### Value loss or Accuracy \n\nThe first evaluation parameter considered here is \u201cLoss\u201d over each epoch of the training run. To calculate value loss over each epoch, Keras provides \u201cval_loss\u201d, which is the average loss after that epoch. The loss observed during the initial epochs at the beginning of training phase is high, but it falls gradually, and that is evident by the screenshots below which shows the run of Architecture in the training phase.\n\n```\nhistory = model.fit_generator(\n    batch_generator(X_train, y_train, 100, 1),\n    steps_per_epoch=300,\n    epochs=10,\n    validation_data=batch_generator(X_valid, y_valid, 100, 0),\n    validation_steps=200,\n    verbose=1,\n    shuffle=1,\n)\n```\n\n![29](https://user-images.githubusercontent.com/91852182/147302882-5ad7e328-a18e-498c-8596-c6cb31573582.png)\n\n### Why We Use ELU Over RELU\n\nWe can have dead relu this is when a node in neural network essentially dies and only feeds a value of zero to nodes which follows it. We will change from relu to elu. Elu function has always a chance to recover and fix it errors means it is in a process of learning and contributing to the model. We will plot the model and then save it accordingly in h5 format for a keras file.\n\n```\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"training\", \"validation\"])\nplt.title(\"Loss\")\nplt.xlabel(\"Epoch\")\n```\n\n<img alt=\"30\" src=\"https://user-images.githubusercontent.com/91852182/147302963-abb40cb3-dd32-4991-8db0-f56e81fee23f.png\">\n\nSave model\n\n```\nmodel.save('model.h5')\n```\n\n## The Connection Part\n\nThis step is required to run the model in the simulated car. For implementing web service using python, we need to install flask. We will use Anaconda environment. Flask is a python micro framework that is used to build the web app. We will use Visual Studio Code.\n\n![31](https://user-images.githubusercontent.com/91852182/147303011-116b5dbd-9892-490e-bc6f-b7644556243a.png)\n\nWe will open the folder where we kept the saved .h5 file, then again open a file but before that, we will install some dependencies. We will also create an anaoconda environment too for doing our work. Click Create Python 3.8.12. with my experience I need to mention that i had some problems with environments, the was a lot of conflicts so I will advise you just to follow my steps exactly the same.\n\n![32](https://user-images.githubusercontent.com/91852182/147303041-3d9a12bb-e42a-4439-8e6d-549122a6e121.png)\n\nAfter we created new env on Python 3.8.12 moving to VScode and opening it under SDC environment. So basically, we now will work with special environment on special Python version. Terminal need to be as well under same.\n\n![34](https://user-images.githubusercontent.com/91852182/147303077-0761ce1c-b713-4237-97d3-d4e272e0adcf.png)\n\n![33](https://user-images.githubusercontent.com/91852182/147303069-3fa65901-85ae-49f0-a11c-bb10025936e5.png)\n\nI will create a list conda ```list -e &gt; requirements.txt``` requirements.txt for you to know what exactly I used there.  It important to use **keras 2.4.3** and be careful with **python-engineio=3.13.0** and lastly most important **python-socketio=4.6.1**(follow requirements.txt). As well you will see drive.py file without this code will not work you can simple copy paste it I will not go deep about this file. \n\nNow when all your requirements are installed, we are ready to run magic code in terminal. In your folder you will see something like I show below.\n\n![35](https://user-images.githubusercontent.com/91852182/147303161-90c4b09a-a180-4c34-b695-b16a7388525e.png)\n\nJust type python ```drive.py model.h5``` in your Terminal.  wait few few second open Udacity Simulator and choose option Autonomous mode. That it, you will see something as I show below.\n\n![36](https://user-images.githubusercontent.com/91852182/147303194-84f45ca9-c61a-4c3d-b054-ec731e2fd79f.png)\n\n## Files\n\nThe project contains the following files:\n- *SelfDrivingCar.py* (script used to create the model and train the model)\n- *drive.py* (script to drive the car - feel free to modify this file)\n- *driving_log.csv* (csv file from simulator - data)\n- *IMG* (folder) (folder with images from simulator - data)\n- *model.h5* (a trained Keras model)\n- *requirements.txt* (important requirements for project)\n- *SelfDrivingCar.ipynb* (full explanation in notebook)\n- *SelfDrivingCar.pdf* (full explanation in PDF)\n\n## Overview\n\nIn this project, we use deep neural networks and convolutional neural networks to clone driving behavior. The model is trained, validated and tested using Keras. The model outputs a steering angle to an autonomous vehicle. The autonomous vehicle is provided as a simulator. Image data and steering angles are used to train a neural network and drive the simulation car autonomously around the track.\nThis project started with training the models and tweaking parameters to get the best performance on the track \nThe use of CNN for getting the spatial features and RNN for the temporal features in the image dataset makes this combination a great fit for building fast and lesser computation required neural networks. Substituting recurrent layers for pooling layers might reduce the loss of information and would be worth exploring in the future projects. \nIt is interesting to find the use of combinations of real world dataset and simulator data to train these models. Then I can get the true nature of how a model can be trained in the simulator and generalized to the real world or vice versa. There are many experimental implementations carried out in the field of self-driving cars and this project contributes towards a significant part of it. \n\n\n## References\n\nGitHub - [https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning](https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning)\n\nGitHub - [https://github.com/SakshayMahna/P4-BehavioralCloning](https://github.com/SakshayMahna/P4-BehavioralCloning)\n\nDeep Learning for Self-Driving Cars - [https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2](https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2)\n\nGitHub - [https://github.com/llSourcell/How_to_simulate_a_self_driving_car](https://github.com/llSourcell/How_to_simulate_a_self_driving_car)\n\nGitHub - [https://github.com/naokishibuya/car-behavioral-cloning](https://github.com/naokishibuya/car-behavioral-cloning)\n\nEnd to End Learning for Self-Driving Cars - [https://arxiv.org/pdf/1604.07316v1.pdf](https://arxiv.org/pdf/1604.07316v1.pdf)\n\nAditya Babhulkar - [https://scholarworks.calstate.edu/downloads/fx719m76s](https://scholarworks.calstate.edu/downloads/fx719m76s)\n\nGitHub - [https://github.com/udacity/self-driving-car-sim](https://github.com/udacity/self-driving-car-sim)\n\n\n\n\n",
  "datasetId": 1807203,
  "datasetSlug": "self-driving-carbehavioural-cloning",
  "hasDatasetSlug": true,
  "ownerUser": "aslanahmedov",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 29346,
  "totalVotes": 63,
  "totalDownloads": 1881,
  "title": "Self Driving Car",
  "hasTitle": true,
  "subtitle": "Behavioural Cloning Complete Guide",
  "hasSubtitle": true,
  "description": "![ezgif com-gif-maker](https://user-images.githubusercontent.com/91852182/147305077-8b86ec92-ed26-43ca-860c-5812fea9b1d8.gif)\n\n# SELF-DRIVING CAR USING UDACITY\u2019S CAR SIMULATOR ENVIRONMENT AND TRAINED BY DEEP NEURAL NETWORKS COMPLETE GUIDE\n\n\n## Table of Contents\n### Introduction\n\n- Problem Definition\n- Solution Approach\n- Technologies Used\n- Convolutional Neural Networks (CNN)\n- Time-Distributed Layers\n\n### Udacity Simulator and Dataset\n### The Training Process\n### Augmentation and image pre-processing\n### Experimental configurations\n### Network architectures\n### Results\n\n- Value loss or Accuracy\n- Why We Use ELU Over RELU\n\n\n### The Connection Part\n### Files\n### Overview\n### References\n\n## Introduction \nSelf-drivi cars has become a trending subject with a significant improvement in the technologies in the last decade. The project purpose is to train a neural network to drive an autonomous car agent on the tracks of Udacity\u2019s Car Simulator environment. Udacity has released the simulator as an open source software and  enthusiasts have hosted a competition (challenge) to teach a car how to drive using only camera images and deep learning. Driving a car in an autonomous manner requires learning to control steering angle, throttle and brakes. Behavioral cloning technique is used to mimic human driving behavior in the training mode on the track. That means a dataset is generated in the simulator by user driven car in training mode, and the deep neural network model then drives the car in autonomous mode. Ultimately, the car was able to run on Track 1 generalizing well. The project aims at reaching the same accuracy on real time data in the future.![6](https://user-images.githubusercontent.com/91852182/147298831-225740f9-6903-4570-8336-0c9f16676456.png)\n\n\n### Problem Definition\n\nUdacity released an open source simulator for self-driving cars to depict a real-time environment. The challenge is to mimic the driving behavior of a human on the simulator with the help of a model trained by deep neural networks. The concept is called Behavioral Cloning, to mimic how a human drives. The simulator contains two tracks and two modes, namely, training mode and autonomous mode. The dataset is generated from the simulator by the user, driving the car in training mode. This dataset is also known as the \u201cgood\u201d driving data. This is followed by testing on the track, seeing how the deep learning model performs after being trained by that user data.\n\n### Solution Approach\n ![1](https://user-images.githubusercontent.com/91852182/147298261-4d57a5c1-1fda-4654-9741-2f284e6d0479.png)\n \n The problem is solved in the following steps: \n \n- The simulator can be used to collect data by driving the car in the training mode using a joystick or keyboard, providing the so called \u201cgood-driving\u201d behavior input data in form of a driving_log (.csv file) and a set of images. The simulator acts as a server and pipes these images and data log to the Python client. \n- The client (Python program) is the machine learning model built using Deep Neural Networks. These models are developed on Keras (a high-level API over Tensorflow). Keras provides sequential models to build a linear stack of network layers. Such models are used in the project to train over the datasets as the second step. Detailed description of CNN models experimented and used can be referred to in the chapter on network architectures. \n- Once the model is trained, it provides steering angles and throttle to drive in an autonomous mode to the server (simulator). \n- These modules, or inputs, are piped back to the server and are used to drive the car autonomously in the simulator and keep it from falling off the track.\n\n### Technologies Used \n\nTechnologies that are used in the implementation of this project and the motivation behind using these are described in this section.\n \nTensorFlow: This an open-source library for dataflow programming. It is widely used for machine learning applications. It is also used as both a math library and for large computation. For this project Keras, a high-level API that uses TensorFlow as the backend is used. Keras facilitate in building the models easily as it more user friendly. \n\nDifferent libraries are available in Python that helps in machine learning projects. Several of those libraries have improved the performance of this project. Few of them are mentioned in this section. First, \u201cNumpy\u201d that provides with high-level math function collection to support multi-dimensional metrices and arrays. This is used for faster computations over the weights (gradients) in neural networks. Second, \u201cscikit-learn\u201d is a machine learning library for Python which features different algorithms and Machine Learning function packages. Another one is OpenCV (Open Source Computer Vision Library) which is designed for computational efficiency with focus on real-time applications. In this project, OpenCV is used for image preprocessing and augmentation techniques. \n\nThe project makes use of Conda Environment which is an open source distribution for Python which simplifies package management and deployment. It is best for large scale data processing. The machine on which this project was built, is a personal computer. \n\n### Convolutional Neural Networks (CNN)\n\nCNN is a type of feed-forward neural network computing system that can be used to learn from input data. Learning is accomplished by determining a set of weights or filter values that allow the network to model the behavior according to the training data. The desired output and the output generated by CNN initialized with random weights will be different. This difference (generated error) is backpropagated through the layers of CNN to adjust the weights of the neurons, which in turn reduces the error and allows us produce output closer to the desired one. \n\nCNN is good at capturing hierarchical and spatial data from images. It utilizes filters that look at regions of an input image with a defined window size and map it to some output. It then slides the window by some defined stride to other regions, covering the whole image. Each convolution filter layer thus captures the properties of this input image hierarchically in a series of subsequent layers, capturing the details like lines in image, then shapes, then whole objects in later layers. CNN can be a good fit to feed the images of a dataset and classify them into their respective classes. \n\n### Time-Distributed Layers\n\nAnother type of layers sometimes used in deep learning networks is a Time- distributed layer. Time-Distributed layers are provided in Keras as wrapper layers. Every temporal slice of an input is applied with this wrapper layer. The requirement for input is that to be at least three-dimensional, first index can be considered as temporal dimension. These Time-Distributed can be applied to a dense layer to each of the timesteps, independently or even used with Convolutional Layers. The way they can be written is also simple in Keras as shown in Figure 1 and Figure 2.\n\n![2](https://user-images.githubusercontent.com/91852182/147298483-4f37a092-7e71-4ce6-9274-9a133d138a4c.png)\n\nFig. 1: TimeDistributed Dense layer\n\n![3](https://user-images.githubusercontent.com/91852182/147298501-6459d968-a279-4140-9be3-2d3ea826d9f6.png)\n\nFig. 2: TimeDistributed Convolution layer\n\n## Udacity Simulator and Dataset \n\nWe will first download the [simulator](https://github.com/udacity/self-driving-car-sim) to start our behavioural training process. Udacity has built a simulator for self-driving cars and made it open source for the enthusiasts, so they can work on something close to a real-time environment. It is built on Unity, the video game development platform. The simulator consists of a configurable resolution and controls setting and is very user friendly. The graphics and input configurations can be changed according to user preference and machine configuration as shown in Figure 3. The user pushes the \u201cPlay!\u201d button to enter the simulator user interface. You can enter the Controls tab to explore the keyboard controls, quite similar to a racing game which can be seen in Figure 4. \n\n![ 4](https://user-images.githubusercontent.com/91852182/147298708-de15ebc5-2482-42f8-b2a2-8d3c59fceff4.png)\n\nFig. 3: Configuration screen                                                                    \n\n![5](https://user-images.githubusercontent.com/91852182/147298712-944e2c2d-e01d-459b-8a7d-3c5471bea179.png)\n\nFig. 4: Controls Configuration\n\nThe first actual screen of the simulator can be seen in Figure 5 and its components are discussed below. The simulator involves two tracks. One of them can be considered as simple and another one as complex that can be evident in the screenshots attached in Figure 6 and Figure 7. The word \u201csimple\u201d here just means that it has fewer curvy tracks and is easier to drive on, refer Figure 6. The \u201ccomplex\u201d track has steep elevations, sharp turns, shadowed environment, and is tough to drive on, even by a user doing it manually. Please refer Figure 6. There are two modes for driving the car in the simulator: (1) Training mode and (2) Autonomous mode. The training mode gives you the option of recording your run and capturing the training dataset. The small red sign at the top right of the screen in the Figure 6 and 7 depicts the car is being driven in training mode. The autonomous mode can be used to test the models to see if it can drive on the track without human intervention. Also, if you try to press the controls to get the car back on track, it will immediately notify you that it shifted to manual controls. The mode screenshot can be as seen in Figure 8. Once we have mastered how the car driven controls in simulator using keyboard keys, then we get started with record button to collect data. We will save the data from it in a specified folder as you can see below.\n\n![6](https://user-images.githubusercontent.com/91852182/147298837-17eecb80-0a3f-4edb-a5f3-050a318f66e0.png)\n\n<img alt=\"7\" src=\"https://user-images.githubusercontent.com/91852182/147298975-e05dc738-2fb7-4dca-a28d-9756285d94cc.png\">\n\nThe simulator\u2019s feature to create your own dataset of images makes it easy to work on the problem. Some reasons why this feature is useful are as follows: \n\n- The simulator has built the driving features in such a way that it simulates that there are three cameras on the car. The three cameras are in the center, right and left on the front of the car, which captures continuously when we record in the training mode. \n- The stream of images is captured, and we can set the location on the disk for saving the data after pushing the record button. The image set are labelled in a sophisticated manner with a prefix of center, left, or right indicating from which camera the image has been captured. \n- Along with the image dataset, it also generates a datalog.csv file. This file contains the image paths with corresponding steering angle, throttle, brakes, and speed of the car at that instance. \n\nA few images from the dataset are shown below .\n\n<img alt=\"8\" src=\"https://user-images.githubusercontent.com/91852182/147299066-17bb3db0-5f1c-44ff-ab7e-41786c243d4f.png\">\n\n# Pic wich i have in folder IMG only for visualization your data will be more than 5000 pic.\n\nA sample of driving_log.csv file is shown in Figure 9.\n\n* *Column 1, 2, 3:*  contains paths to the dataset images of center, right and left respectively Column 4: contains the steering angle \n* *Column 4:*  value as 0 depicts straight, positive value is right turn and negative value is left turn. \n* *Column 5:*  contains the throttle or acceleration at that instance \n* *Column 6:*  contains the brakes or deceleration at that instance \n* *Column 7:*  contains the speed of the vehicle \n\n![9](https://user-images.githubusercontent.com/91852182/147299491-77d853c4-604b-42ef-8db4-2469a2993e00.png)\n\nFig 9 . driving_log.csv\n\n## The Training Process\n\nFor the process of getting the self driving car working, we have to upload the images that we recorded using the simulator. First of all, we will open [GitHub Desktop.](https://desktop.github.com/)If we do not have an account, we will create a new one. With that, we will create a new repository.\n\n![10](https://user-images.githubusercontent.com/91852182/147299586-a4afa457-d6ed-4383-a8d9-adf6fbcded47.png)\n\nWe will be using [Google Colab](https://colab.research.google.com/) for doing the training process or [Kaggle](https://www.kaggle.com/).\nWe will open a new python3 notebook and get started. Next, we will git clone the repo.\n\n```!git clone https://github.com/Asikpalysik/Self-Driving-Car.git```\n \nWe will now import all the libraries needed for training process. It will use Tensorflow backend and keras at frontend.\n \n```\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom imgaug import augmenters as iaa\nimport cv2\nimport pandas as pd\nimport ntpath\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\nWe wil use datadir as the name given to the folder itself and take the parameters itself. Using head, we will show the first five values for the CSV on the desired format\n\n```\ndir = \"/Users/asik/Desktop/SelfDrivingCar\"\ncolumns = [\"center\", \"left\", \"right\", \"steering\", \"throttle\", \"reverse\", \"speed\"]\ndata = pd.read_csv(os.path.join(dir, \"driving_log.csv\"), names=columns)\npd.set_option(\"display.max_colwidth\", -1)\ndata.head()\n```\n\n![11](https://user-images.githubusercontent.com/91852182/147300673-09faac02-c98a-4139-9151-b5da96b74593.png)\n\nAs this is picking up the entire path from the local machine, we need to use ntpath function to get the network path assigned. We will declare a name path_leaf and assign accordingly.\n\n```\ndef pathleaf(path):\n    head, tail = ntpath.split(path)\n    return tail\n    \ndata[\"center\"] = data[\"center\"].apply(pathleaf)\ndata[\"left\"] = data[\"left\"].apply(pathleaf)\ndata[\"right\"] = data[\"right\"].apply(pathleaf)\ndata.head()\n```\n\n![12](https://user-images.githubusercontent.com/91852182/147300702-81a81f27-41f8-49a6-843b-67838b1bcb84.png)\n\nWe will bin the number of values where the number will be equal to 25 (odd number aimed to get center distribution). We will see the histogram using the np.histogram option on data frame \u2018steering\u2019, we will divide it to the number of bins. We keep samples at 400 and then we draw a line. We see the data is centered along the middle that is 0.\n\n```\nnum_bins = 25\nsamples_per_bin = 400\nhist, bins = np.histogram(data[\"steering\"], num_bins)\nprint(bins)\n```\n\n![13](https://user-images.githubusercontent.com/91852182/147300730-c543c7cd-9ced-4019-9aa8-555e44b123cc.png)\n\nPlot on it\n\n```\ncenter = (bins[:-1] + bins[1:]) * 0.5\nplt.bar(center, hist, width=0.05)\nplt.plot(\n    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n    (samples_per_bin, samples_per_bin),\n)\n```\n\n<img alt=\"14\" src=\"https://user-images.githubusercontent.com/91852182/147301498-cb9aac07-837e-4f66-8825-36608843dab4.png\">\n\n```\nprint(\"Total Data:\", len(data))\n```\n-&gt;&gt;Total Data: 1795\n\n\nWe wil specify a variable remove_list.We will specify samples we want to remove using looping construct through every single bin we will iterate through all the steering data. We will shuffle the data and romve some from it as it is now uniformly structured after shuffling.The output will be the distribution of steering angle that are much more uniform. There are significant amount of left steering angle and right steering angle eliminating the bias to drive straight all the time.\n\n```\nremove_list = []\nfor j in range(num_bins):\n    list_ = []\n    for i in range(len(data[\"steering\"])):\n        if data[\"steering\"][i] &gt;= bins[j] and data[\"steering\"][i] &lt;= bins[j + 1]:\n            list_.append(i)\n    list_ = shuffle(list_)\n    list_ = list_[samples_per_bin:]\n    remove_list.extend(list_)\nprint(\"Removed:\", len(remove_list))\n```\n-&gt;&gt; Removed: 945\n\n```\ndata.drop(data.index[remove_list], inplace=True)\nprint(\"Remaining:\", len(data))\n-&gt;&gt;Remaining: 850\n```\n\nPlot on it\n\n```\nhist, _ = np.histogram(data[\"steering\"], (num_bins))\nplt.bar(center, hist, width=0.05)\nplt.plot(\n    (np.min(data[\"steering\"]), np.max(data[\"steering\"])),\n    (samples_per_bin, samples_per_bin),\n)\n```\n\n<img alt=\"15\" src=\"https://user-images.githubusercontent.com/91852182/147301598-185b5e70-17f8-41db-81d3-50cfc5db3902.png\">\n\n```\nprint(data.iloc[1])\n```\n\n![16](https://user-images.githubusercontent.com/91852182/147301621-fcfb3f40-607f-4216-af9e-d82967e24bc9.png)\n\nWe will now load the image into array to manipulate them accordingly. We will define a function named locd_img_steering. We will have image path as empty list and steering as empty list and then loop through. We use iloc selector as data frame based on the specific index, we will use cut data for now.\n\n```\ndef load_img_steering(datadir, df):\n    image_path = []\n    steering = []\n    for i in range(len(data)):\n        indexed_data = data.iloc[i]\n        center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]\n        image_path.append(os.path.join(datadir, center.strip()))\n        steering.append(float(indexed_data[3]))\n        image_path.append(os.path.join(datadir, left.strip()))\n        steering.append(float(indexed_data[3]) + 0.15)\n        image_path.append(os.path.join(datadir, right.strip()))\n        steering.append(float(indexed_data[3]) - 0.15)\n    image_paths = np.asarray(image_path)\n    steerings = np.asarray(steering)\n    return image_paths, steerings\n```\n\nWe will be splitting the image path as well as storing arrays accordingly.\n\n```\nimage_paths, steerings = load_img_steering(dir + \"/IMG\", data)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    image_paths, steerings, test_size=0.2, random_state=6\n)\nprint(\"Training Samples: {}\\nValid Samples: {}\".format(len(X_train), len(X_valid)))\n```\n-&gt;&gt;Training Samples: 2040 \n\n-&gt;&gt;Valid Samples: 510\n\nWe will have the histograms now.\n\n```\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].hist(y_train, bins=num_bins, width=0.05, color=\"blue\")\naxes[0].set_title(\"Training set\")\naxes[1].hist(y_valid, bins=num_bins, width=0.05, color=\"red\")\naxes[1].set_title(\"Validation set\")\n```\n\n<img alt=\"17\" src=\"https://user-images.githubusercontent.com/91852182/147301799-0e3ca6a5-6f4e-41a9-b2c5-7bf5b1dad400.png\">\n\n## Augmentation and image pre-processing\nThe biggest challenge was generalizing the behavior of the car on Track_2 which it was never trained for. In a real-life situation, we can never train a self-driving car model for every track possible, as the data will be too huge to process. Also, it is not possible to gather the dataset for all the weather conditions and roads. Thus, there is a need to come up with an idea of generalizing the behavior on different tracks. This problem is solved using image preprocessing and augmentation techniques.\n\n- Zoom \n\n\nThe images in the dataset have relevant features in the lower part where the road is visible. The external environment above a certain image portion will never be used to determine the output and thus can be cropped. Approximately, 30% of the top portion of the image is cut and passed in the training set. The snippet of code and transformation of an image after cropping and resizing it to original image can be seen in  below.\n\n<img alt=\"18\" src=\"https://user-images.githubusercontent.com/91852182/147301992-fdc9d029-3a33-4f2f-8dc2-222067a07a0a.png\">\n\n```\ndef zoom(image):\n    zoom = iaa.Affine(scale=(1, 1.3))\n    image = zoom.augment_image(image)\n    return image\n\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\nzoomed_image = zoom(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(zoomed_image)\naxs[1].set_title(\"Zoomed Image\")\n```\n\n- Flip (horizontal) \n\nThe image is flipped horizontally (i.e. a mirror image of the original image is passed to the dataset). The motive behind this is that the model gets trained for similar kinds of turns on opposite sides too. This is important because Track 1 includes only left turns. The snippet of code and transformation of an image after flipping it can be seen in below. \n\n<img alt=\"19\" src=\"https://user-images.githubusercontent.com/91852182/147302076-3bd4311e-79b2-4750-84a7-c1061ffcdae3.png\">\n\n```\ndef random_flip(image, steering_angle):\n    image = cv2.flip(image, 1)\n    steering_angle = -steering_angle\n    return image, steering_angle\n\nrandom_index = random.randint(0, 1000)\nimage = image_paths[random_index]\nsteering_angle = steerings[random_index]\n\noriginal_image = mpimg.imread(image)\nflipped_image, flipped_steering_angle = random_flip(original_image, steering_angle)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image - \" + \"Steering Angle:\" + str(steering_angle))\n\naxs[1].imshow(flipped_image)\naxs[1].set_title(\"Flipped Image - \" + \"Steering Angle:\" + str(flipped_steering_angle))\n```\n\n- Shift (horizontal/vertical)\n\n\nThe image is shifted by a small amount, it is vertical shift and horizontal shift as below.\n\n\n<img alt=\"20\" src=\"https://user-images.githubusercontent.com/91852182/147302200-34bae3a8-2236-42d7-819d-ea694a382a02.png\">\n\n```\ndef pan(image):\n    pan = iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n    image = pan.augment_image(image)\n    return image\n\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\npanned_image = pan(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(panned_image)\naxs[1].set_title(\"Panned Image\")\n```\n\n- Brightness \n\n\nTo generalize to the weather conditions with bright sunny day or cloudy, lowlight conditions, the brightness augmentation can prove to be very useful. The code snippet and increase of brightness can be seen below. Similarly, I have randomly also lowered down the level of brightness for other conditions. \n\n<img alt=\"21\" src=\"https://user-images.githubusercontent.com/91852182/147302277-defccf7a-43f4-459c-a0db-536f01b77110.png\">\n\n```\ndef random_brightness(image):\n    brightness = iaa.Multiply((0.2, 1.2))\n    image = brightness.augment_image(image)\n    return image\n\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\nbrightness_altered_image = random_brightness(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(brightness_altered_image)\naxs[1].set_title(\"Brightness altered image \")\n```\n\nTo have a look what we have at this moment\n\n```\ndef random_augment(image, steering_angle):\n    image = mpimg.imread(image)\n    if np.random.rand() &lt; 0.5:\n        image = pan(image)\n    if np.random.rand() &lt; 0.5:\n        image = zoom(image)\n    if np.random.rand() &lt; 0.5:\n        image = random_brightness(image)\n    if np.random.rand() &lt; 0.5:\n        image, steering_angle = random_flip(image, steering_angle)\n    return image, steering_angle\n\nncol = 2\nnrow = 10\n\nfig, axs = plt.subplots(nrow, ncol, figsize=(15, 50))\nfig.tight_layout()\n\nfor i in range(10):\n    randnum = random.randint(0, len(image_paths) - 1)\n    random_image = image_paths[randnum]\n    random_steering = steerings[randnum]\n    original_image = mpimg.imread(random_image)\n    augmented_image, steering = random_augment(random_image, random_steering)\n    axs[i][0].imshow(original_image)\n    axs[i][0].set_title(\"Original Image\")\n\n    axs[i][1].imshow(augmented_image)\n    axs[i][1].set_title(\"Augmented Image\")\n```\n\n![23](https://user-images.githubusercontent.com/91852182/147302403-33b0de7e-5cab-46b1-a16a-77321cec9cbe.png)\n![24](https://user-images.githubusercontent.com/91852182/147302408-aadfd20d-6cc0-43cf-9da7-39e1ada3fe9a.png)\n\n\nI continued by doing some image processing. I cropped the image to remove the unnecessary features, changes the images to YUV format, used gaussian blur, decreased the size for easier processing and normalized the values.\n\n```\ndef img_preprocess(img):\n    ## Crop image to remove unnecessary features\n    img = img[60:135, :, :]\n    ## Change to YUV image\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n    ## Gaussian blur\n    img = cv2.GaussianBlur(img, (3, 3), 0)\n    ## Decrease size for easier processing\n    img = cv2.resize(img, (200, 66))\n    ## Normalize values\n    img = img / 255\n    return img\n```\n\nTo compare and visualize I plotted the original and the pre-processed image.\n\n```\nimage = image_paths[100]\noriginal_image = mpimg.imread(image)\npreprocessed_image = img_preprocess(original_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(original_image)\naxs[0].set_title(\"Original Image\")\n\naxs[1].imshow(preprocessed_image)\naxs[1].set_title(\"Preprocessed Image\")\n```\n\n<img alt=\"25\" src=\"https://user-images.githubusercontent.com/91852182/147302472-ac64e7d7-1ec5-403a-82ff-046258584145.png\">\n\n```\ndef batch_generator(image_paths, steering_ang, batch_size, istraining):\n    while True:\n        batch_img = []\n        batch_steering = []\n\n        for i in range(batch_size):\n            random_index = random.randint(0, len(image_paths) - 1)\n\n            if istraining:\n                im, steering = random_augment(\n                    image_paths[random_index], steering_ang[random_index]\n                )\n\n            else:\n                im = mpimg.imread(image_paths[random_index])\n                steering = steering_ang[random_index]\n\n            im = img_preprocess(im)\n            batch_img.append(im)\n            batch_steering.append(steering)\n\n        yield (np.asarray(batch_img), np.asarray(batch_steering))\n```\n\nSo far so good. Next, I converted all the images into numpy array.\n\n```\nx_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))\nx_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n\naxs[0].imshow(x_train_gen[0])\naxs[0].set_title(\"Training Image\")\n\naxs[1].imshow(x_valid_gen[0])\naxs[1].set_title(\"Validation Image\")\n```\n\n<img alt=\"26\" src=\"https://user-images.githubusercontent.com/91852182/147302514-54b40af4-baea-491e-ba7b-73e6d752872e.png\">\n\n## Experimental configurations\n\nConfigurations used to set up the models for training the Python Client to provide the Neural Network outputs that drive the car on the simulator. The tweaking of parameters and rigorous experiments were tried to reach the best combination. Though each of the models had their unique behaviors and differed in their performance with each tweak, the following combination of configuration can be considered as the optimal: \n\n- The sequential models built on Keras with deep neural network layers are used to train the data. \n- Models are only trained using the dataset from Track 1. \n- 80% of the dataset is used for training, 20% is used for testing. \n- Epochs = 10, i.e. number of iterations or passes through the complete dataset. Experimented with larger number of epochs also, but the model tried to \u201coverfit\u201d. In other words, the model learns the details in the training data too well, while impacting the performance on new dataset. \n- Batch-size = 100, i.e. number of image samples propagated through the network, like a subset of data as complete dataset is too big to be passed all at once.\n- Learning rate = 0.0001, i.e. how the coefficients of the weights or gradients change in the network. \n\nThere are different combinations of Convolution layer, Time-Distributed layer, MaxPooling layer, Flatten, Dropout, Dense and so on, that can be used to implement the Neural Network models. \n\n## Network architectures\n\nThe design of the network is based on the NVIDIA model, which has been used by NVIDIA for the end-to-end self driving test. As such, it is well suited for the project.It is a deep convolution network which works well with supervised image classification / regression problems. As the NVIDIA model is well documented, I was able to focus how to adjust the training images to produce the best result with some adjustments to the model to avoid overfitting and adding non-linearity to improve the prediction.\n\nI've added the following adjustments to the model.\n\n- I used Lambda layer to normalized input images to avoid saturation and make gradients work better.\n- I've added an additional dropout layer to avoid overfitting after the convolution layers.\n- I've also included ELU for activation function for every layer except for the output layer to introduce non-linearity.\n\nIn the end, the model looks like as follows:\n\n- Image normalization\n- Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n- Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n- Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n- Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n- Drop out (0.5)\n- Fully connected: neurons: 100, activation: ELU\n- Fully connected: neurons: 50, activation: ELU\n- Fully connected: neurons: 10, activation: ELU\n- Fully connected: neurons: 1 (output)\n\n![27](https://user-images.githubusercontent.com/91852182/147302681-661ef73d-8cb1-452f-bbcd-323052972189.png)\n\nAs per the NVIDIA model, the convolution layers are meant to handle feature engineering and the fully connected layer for predicting the steering angle. However, as stated in the NVIDIA document, it is not clear where to draw such a clear distinction. Overall, the model is very functional to clone the given steering behavior. The below is a model structure output from the Keras which gives more details on the shapes and the number of parameters.\n\nWe will design our Model architecture. We have to classify the traffic signs too that's why we need to shift from Lenet 5 model to NVDIA model. With behavioural cloning, our dataset is much more complex then any dataset we have used. We are dealing with images that have (200,66) dimensions. Our current datset has 5386 images to train with but MNSIT has around 60,000 images to train with. Our behavioural cloning code has simply has to return appropriate steering angle which is a regression type example. For these things, we need a more advanced model which is provided by nvdia and known as nvdia model.\n\nFor defining the model architecture, we need to define the model object. Normalization state can be skipped as we have already normalized it. We will add the convolution layer. As compared to the model, we will organize accordingly. The Nvdia model uses 24 filters in the layer along with a kernel of size 5,5. We will introduce sub sampling. The function reflects to stride length of the kernel as it processes through an image, we have large images. Horizontal movement with 2 pixels at a time, similarly vertical movement to 2 pixels at a time. As this is the first layer, we have to define input shape of the model too i.e., (66,200,3) and the last function is an activation function that is \u201celu\u201d.\n\nRevisting the model, we see that our second layer has 36 filters with kernel size (5,5) same subsampling option with stride length of (2,2) and conclude this layer with activation \u2018elu\u2019.\n\nAccording to Nvdia model, it shows we have 3 more layers in the convolutional neural network. With 48 filters, with 64 filters (3,3) kernel 64 filters (3,3) kernel Dimensions have been reduced significantly so for that we will remove subsampling from 4th and 5th layer.\n\nNext we add a flatten layer. We will take the output array from previous convolution neural network to convert it into a one dimensional array so that it can be fed to fully connected layer to follow.\n\nOur last convolution layer outputs an array shape of (1,18) by 64.\n\nWe end the architecture of Nvdia model with a dense layer containing a single output node which will output the predicted steering angle for our self driving car. Now we will use model.compile() to compile our architecture as this is a regression type example the metrics that we will be using will be mean squared error and optimize as Adam. We will be using relatively a low learning rate that it can help on accuracy. We will use dropout layer to avoid overfitting the data. Dropout Layer sets the input of random fraction of nodes to \u201c0\u201d during each update. During this, we will generate the training data as it is forced to use a variety of combination of nodes to learn from the same data. We will have to separate the convolution layer from fully connected layer with a factor of 0.5 is added so it converts 50 percent of the input to 0. We Will define the model by calling the nvdia model itself. Now we will have the model training process.To define training parameters, we will use model.fit(), we will import our training data X_Train, training data -&gt;y_train, we have less data on the datasets we will require more epochs to be effective. We will use validation data and then use Batch size.\n\n\n```\ndef NvidiaModel():\n  model = Sequential()\n  model.add(Convolution2D(24,(5,5),strides=(2,2),input_shape=(66,200,3),activation=\"elu\"))\n  model.add(Convolution2D(36,(5,5),strides=(2,2),activation=\"elu\"))\n  model.add(Convolution2D(48,(5,5),strides=(2,2),activation=\"elu\")) \n  model.add(Convolution2D(64,(3,3),activation=\"elu\"))   \n  model.add(Convolution2D(64,(3,3),activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Flatten())\n  model.add(Dense(100,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Dense(50,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Dense(10,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  model.add(Dense(1))\n  model.compile(optimizer=Adam(lr=1e-3),loss=\"mse\")\n  return model\n```\n\n```\nmodel = NvidiaModel()\nprint(model.summary())\n```\n\n![28](https://user-images.githubusercontent.com/91852182/147302750-f96a9310-e1d7-475d-bb55-d02c457d2139.png)\n\n## Results\n\nThe following results were observed for described architectures. I had to come up with two different performance metrics. \n- Value loss or Accuracy (computed during training phase) \n- Generalization on Track 1 (drive performance)\n\n### Value loss or Accuracy \n\nThe first evaluation parameter considered here is \u201cLoss\u201d over each epoch of the training run. To calculate value loss over each epoch, Keras provides \u201cval_loss\u201d, which is the average loss after that epoch. The loss observed during the initial epochs at the beginning of training phase is high, but it falls gradually, and that is evident by the screenshots below which shows the run of Architecture in the training phase.\n\n```\nhistory = model.fit_generator(\n    batch_generator(X_train, y_train, 100, 1),\n    steps_per_epoch=300,\n    epochs=10,\n    validation_data=batch_generator(X_valid, y_valid, 100, 0),\n    validation_steps=200,\n    verbose=1,\n    shuffle=1,\n)\n```\n\n![29](https://user-images.githubusercontent.com/91852182/147302882-5ad7e328-a18e-498c-8596-c6cb31573582.png)\n\n### Why We Use ELU Over RELU\n\nWe can have dead relu this is when a node in neural network essentially dies and only feeds a value of zero to nodes which follows it. We will change from relu to elu. Elu function has always a chance to recover and fix it errors means it is in a process of learning and contributing to the model. We will plot the model and then save it accordingly in h5 format for a keras file.\n\n```\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"training\", \"validation\"])\nplt.title(\"Loss\")\nplt.xlabel(\"Epoch\")\n```\n\n<img alt=\"30\" src=\"https://user-images.githubusercontent.com/91852182/147302963-abb40cb3-dd32-4991-8db0-f56e81fee23f.png\">\n\nSave model\n\n```\nmodel.save('model.h5')\n```\n\n## The Connection Part\n\nThis step is required to run the model in the simulated car. For implementing web service using python, we need to install flask. We will use Anaconda environment. Flask is a python micro framework that is used to build the web app. We will use Visual Studio Code.\n\n![31](https://user-images.githubusercontent.com/91852182/147303011-116b5dbd-9892-490e-bc6f-b7644556243a.png)\n\nWe will open the folder where we kept the saved .h5 file, then again open a file but before that, we will install some dependencies. We will also create an anaoconda environment too for doing our work. Click Create Python 3.8.12. with my experience I need to mention that i had some problems with environments, the was a lot of conflicts so I will advise you just to follow my steps exactly the same.\n\n![32](https://user-images.githubusercontent.com/91852182/147303041-3d9a12bb-e42a-4439-8e6d-549122a6e121.png)\n\nAfter we created new env on Python 3.8.12 moving to VScode and opening it under SDC environment. So basically, we now will work with special environment on special Python version. Terminal need to be as well under same.\n\n![34](https://user-images.githubusercontent.com/91852182/147303077-0761ce1c-b713-4237-97d3-d4e272e0adcf.png)\n\n![33](https://user-images.githubusercontent.com/91852182/147303069-3fa65901-85ae-49f0-a11c-bb10025936e5.png)\n\nI will create a list conda ```list -e &gt; requirements.txt``` requirements.txt for you to know what exactly I used there.  It important to use **keras 2.4.3** and be careful with **python-engineio=3.13.0** and lastly most important **python-socketio=4.6.1**(follow requirements.txt). As well you will see drive.py file without this code will not work you can simple copy paste it I will not go deep about this file. \n\nNow when all your requirements are installed, we are ready to run magic code in terminal. In your folder you will see something like I show below.\n\n![35](https://user-images.githubusercontent.com/91852182/147303161-90c4b09a-a180-4c34-b695-b16a7388525e.png)\n\nJust type python ```drive.py model.h5``` in your Terminal.  wait few few second open Udacity Simulator and choose option Autonomous mode. That it, you will see something as I show below.\n\n![36](https://user-images.githubusercontent.com/91852182/147303194-84f45ca9-c61a-4c3d-b054-ec731e2fd79f.png)\n\n## Files\n\nThe project contains the following files:\n- *SelfDrivingCar.py* (script used to create the model and train the model)\n- *drive.py* (script to drive the car - feel free to modify this file)\n- *driving_log.csv* (csv file from simulator - data)\n- *IMG* (folder) (folder with images from simulator - data)\n- *model.h5* (a trained Keras model)\n- *requirements.txt* (important requirements for project)\n- *SelfDrivingCar.ipynb* (full explanation in notebook)\n- *SelfDrivingCar.pdf* (full explanation in PDF)\n\n## Overview\n\nIn this project, we use deep neural networks and convolutional neural networks to clone driving behavior. The model is trained, validated and tested using Keras. The model outputs a steering angle to an autonomous vehicle. The autonomous vehicle is provided as a simulator. Image data and steering angles are used to train a neural network and drive the simulation car autonomously around the track.\nThis project started with training the models and tweaking parameters to get the best performance on the track \nThe use of CNN for getting the spatial features and RNN for the temporal features in the image dataset makes this combination a great fit for building fast and lesser computation required neural networks. Substituting recurrent layers for pooling layers might reduce the loss of information and would be worth exploring in the future projects. \nIt is interesting to find the use of combinations of real world dataset and simulator data to train these models. Then I can get the true nature of how a model can be trained in the simulator and generalized to the real world or vice versa. There are many experimental implementations carried out in the field of self-driving cars and this project contributes towards a significant part of it. \n\n\n## References\n\nGitHub - [https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning](https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning)\n\nGitHub - [https://github.com/SakshayMahna/P4-BehavioralCloning](https://github.com/SakshayMahna/P4-BehavioralCloning)\n\nDeep Learning for Self-Driving Cars - [https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2](https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2)\n\nGitHub - [https://github.com/llSourcell/How_to_simulate_a_self_driving_car](https://github.com/llSourcell/How_to_simulate_a_self_driving_car)\n\nGitHub - [https://github.com/naokishibuya/car-behavioral-cloning](https://github.com/naokishibuya/car-behavioral-cloning)\n\nEnd to End Learning for Self-Driving Cars - [https://arxiv.org/pdf/1604.07316v1.pdf](https://arxiv.org/pdf/1604.07316v1.pdf)\n\nAditya Babhulkar - [https://scholarworks.calstate.edu/downloads/fx719m76s](https://scholarworks.calstate.edu/downloads/fx719m76s)\n\nGitHub - [https://github.com/udacity/self-driving-car-sim](https://github.com/udacity/self-driving-car-sim)\n\n\n\n\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "video games",
    "earth and nature",
    "robotics",
    "artificial intelligence",
    "computer science",
    "programming",
    "dnn",
    "cnn",
    "keras"
  ],
  "licenses": [
    {
      "nameNullable": "other",
      "name": "other",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}

DATASET FILES CONTENTS:

Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\drive.py
b"import socketio\r\nimport eventlet\r\nimport numpy as np\r\nfrom flask import Flask\r\nfrom keras.models import load_model\r\nimport base64\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\nimport cv2\r\n \r\nsio = socketio.Server()\r\n \r\napp = Flask(__name__) #'__main__'\r\nspeed_limit = 10\r\ndef img_preprocess(img):\r\n    img = img[60:135,:,:]\r\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\r\n    img = cv2.GaussianBlur(img,  (3, 3), 0)\r\n    img = cv2.resize(img, (200, 66))\r\n    img = img/255\r\n    return img\r\n \r\n \r\n@sio.on('telemetry')\r\ndef telemetry(sid, data):\r\n    speed = float(data['speed'])\r\n    image = Image.open(BytesIO(base64.b64decode(data['image'])))\r\n    image = np.asarray(image)\r\n    image = img_preprocess(image)\r\n    image = np.array([image])\r\n    steering_angle = float(model.predict(image))\r\n    throttle = 1.0 - speed/speed_limit\r\n    print('{} {} {}'.format(steering_angle, throttle, speed))\r\n    send_control(steering_angle, throttle)\r\n \r\n \r\n \r\n@sio.on('connect')\r\ndef connect(sid, environ):\r\n    print('Connected')\r\n "
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\driving_log.csv
b'/Users/asik/Desktop/Self Driving Car/IMG/center_2021_12_19_18_46_10_430.jpg, /Users/asik/Desktop/Self Driving Car/IMG/left_2021_12_19_18_46_10_430.jpg, /Users/asik/Desktop/Self Driving Car/IMG/right_2021_12_19_18_46_10_430.jpg, 0, 0, 0, 7.896055E-05\n/Users/asik/Desktop/Self Driving Car/IMG/center_2021_12_19_18_46_10_551.jpg, /Users/asik/Desktop/Self Driving Car/IMG/left_2021_12_19_18_46_10_551.jpg, /Users/asik/Desktop/Self Driving Car/IMG/right_2021_12_19_18_46_10_551.jpg, 0, 0, 0, 7.928215E-05\n/Users/asik/Desktop/Self Driving Car/IMG/center_2021_12_19_18_46_10_723.jpg, /Users/asik/Desktop/Self Driving Car/IMG/left_2021_12_19_18_46_10_723.jpg, /Users/asik/Desktop/Self Driving Car/IMG/right_2021_12_19_18_46_10_723.jpg, 0, 0, 0, 8.011018E-05\n/Users/asik/Desktop/Self Driving Car/IMG/center_2021_12_19_18_46_10_830.jpg, /Users/asik/Desktop/Self Driving Car/IMG/left_2021_12_19_18_46_10_830.jpg, /Users/asik/Desktop/Self Driving Car/IMG/right_2021_12_19_18_46_10_830.jpg, 0, 0, 0, 7.961145E-05\n/Users/asik/Desktop/Self'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\model.h5
b'\x89HDF\r\n\x1a\n\x00\x00\x00\x00\x00\x08\x08\x00\x04\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xff\xff\xff\xff\xff\xff\xff\xff\xe8\x85/\x00\x00\x00\x00\x00\xff\xff\xff\xff\xff\xff\xff\xff\x00\x00\x00\x00\x00\x00\x00\x00`\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x88\x00\x00\x00\x00\x00\x00\x00\xa8\x02\x00\x00\x00\x00\x00\x00\x01\x00\x07\x00\x01\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x10\x00\x10\x00\x00\x00\x00\x00 \x03\x00\x00\x00\x00\x00\x00h\x01\x00\x00\x00\x00\x00\x00TREE\x00\x00\x01\x00\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00(\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00HEAP\x00\x00\x00\x00X\x00\x00\x00\x00\x00\x00\x000\x00\x00\x00\x00\x00\x00\x00\xc8\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00model_weights\x00\x00\x00optimizer_weights\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00(\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x11\x00\x10\x00\x00\x00\x00\x00\x88\x00\x00\x00\x00\x00\x00\x00\xa8\x02\x00\x00\x00\x00\x00\x00\x0c\x00H\x00\x04\x00\x00\x00\x01\x00\x0e\x00\x14\x00\x08\x00keras_version\x00\x00\x00\x19\x01\x01\x00\x10\x00\x00\x00\x10\x00\x00\x00\x01\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x0c\x00@\x00\x04\x00\x00\x00\x01\x00\x08\x00\x14\x00\x08\x00backend\x00\x19\x01\x01\x00\x10\x00\x00\x00\x10\x00\x00\x00\x01\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\n\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x0c\x00H\x00\x04\x00\x00\x00\x01\x00\r\x00\x14\x00\x08\x00model_config\x00\x00\x00\x00\x19\x01\x00\x00\x10\x00\x00\x00\x10\x00\x00\x00\x01\x00\x00\x00'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\README.md
b'![ezgif com-gif-maker](https://user-images.githubusercontent.com/91852182/147305077-8b86ec92-ed26-43ca-860c-5812fea9b1d8.gif)\n\n# SELF-DRIVING CAR USING UDACITY\xe2\x80\x99S CAR SIMULATOR ENVIRONMENT AND TRAINED BY DEEP NEURAL NETWORKS COMPLETE GUIDE\n\n\n## Table of Contents\n### Introduction\n\n- Problem Definition\n- Solution Approach\n- Technologies Used\n- Convolutional Neural Networks (CNN)\n- Time-Distributed Layers\n\n### Udacity Simulator and Dataset\n### The Training Process\n### Augmentation and image pre-processing\n### Experimental configurations\n### Network architectures\n### Results\n\n- Value loss or Accuracy\n- Why We Use ELU Over RELU\n\n\n### The Connection Part\n### Files\n### Overview\n### References\n\n## Introduction \nSelf-drivi cars has become a trending subject with a significant improvement in the technologies in the last decade. The project purpose is to train a neural network to drive an autonomous car agent on the tracks of Udacity\xe2\x80\x99s Car Simulator environment. Udacity has released the simulator as an open source so'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\requirements.txt
b'# This file may be used to create an environment using:\n# $ conda create --name <env> --file <this file>\n# platform: osx-64\nabsl-py=1.0.0=pypi_0\nappnope=0.1.2=py38hecd8cb5_1001\nastunparse=1.6.3=pypi_0\nautopep8=1.6.0=pypi_0\nbackcall=0.2.0=pyhd3eb1b0_0\nbidict=0.21.4=pypi_0\nblas=1.0=mkl\nbrotli=1.0.9=hb1e8313_2\nca-certificates=2021.10.26=hecd8cb5_2\ncachetools=4.2.4=pypi_0\ncertifi=2021.10.8=py38hecd8cb5_0\ncharset-normalizer=2.0.9=pypi_0\nclick=8.0.3=pypi_0\ncycler=0.11.0=pyhd3eb1b0_0\ndebugpy=1.5.1=py38he9d5cce_0\ndecorator=5.1.0=pyhd3eb1b0_0\ndnspython=2.1.0=pypi_0\nentrypoints=0.3=py38_0\neventlet=0.33.0=pypi_0\nflask=2.0.2=pypi_0\nflatbuffers=2.0=pypi_0\nfonttools=4.25.0=pyhd3eb1b0_0\nfreetype=2.11.0=hd8bbffd_0\ngast=0.4.0=pypi_0\ngiflib=5.2.1=haf1e3a3_0\ngoogle-auth=2.3.3=pypi_0\ngoogle-auth-oauthlib=0.4.6=pypi_0\ngoogle-pasta=0.2.0=pypi_0\ngreenlet=1.1.2=pypi_0\ngrpcio=1.43.0=pypi_0\nh5py=3.6.0=pypi_0\nidna=3.3=pypi_0\nimageio=2.13.5=pypi_0\nimgaug=0.4.0=pypi_0\nimportlib-metadata=4.10.0=pypi_0\nintel-openmp=2021.4.0=hecd8cb5_3538\ni'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\Self Driving Car.pdf
b'%PDF-1.3\n%\xc4\xe5\xf2\xe5\xeb\xa7\xf3\xa0\xd0\xc4\xc6\n3 0 obj\n<< /Filter /FlateDecode /Length 3394 >>\nstream\nx\x01\xc5\\M\x8f\xdb8\x12\xbd\xebW(\xee\x89[\x1d\xc7jR\xa2(*\x93\xec\xcc$\x93\xdd\xc5\xde\x06\xe8\xdbfO\x01\xf6\xb0\xc0\x1c\x06\xf9\xff\xc0\xbe\xe2\xb7E\xb6E\xc9mw\xf7A\x96L\x15Ud\xbdW\xc5*\xca\x7f\xd5\x7f\xd4\x7f\xd5\x8f_~\xf0\xfa\xfb\x8f\x9a\xe9\xff\x1f\xdfq\x89\xb5\x9d0\xe7\xf4\x81\xabZv\xbc\x9d\xba\xfa\xfb\x9f\xf5\xe7\'|\xcb\x18.?}\xafU\xaf[\xe1\xd0uC=\xf4S\xf5\xf4g\xfd\xf8\xf4\xd4\xd5\xbc~\xfao\xdd\xbc\xd9\xdd\xfd\xf4P?\xfd\xaf\xfe\xfa\xa4\xbb:/7H\x13}\xdf\n.\xc6\x8c\xc8\xb7\xe5\xf2\xe89{<H\xfc\x9cB\xf2\x96\xcbA\xa4\x92\xff]7\xfb\xfbo\xcd\x03\x1e\xbd\xf9\xf6`?\xbc;\xbc\x7f\xa8\xbb\xba9\xb6\xf7\xe6\xc3\xe3\x1b\xd3\xc0\xb4s_?\xee\xdb#\xee\xa8p+\xe3\x9d\xbe\xc3\xb4\x8b\xef\xfeO\xfd\xf4\xaf\xb2\x810\x03\\\x9d<8\r\xb0\x10}}2\xc0x\xe67\xf6I\xfb\x87\xfa\x88\xee\x1f\xcd\xe1\xae5G&\xe8X5\xf7\xe6\x14\x8f\x836\xbbw\xe6\xec\xdb\x83=\x1ael\x1b}G\xdd\xd86V\xac\xbb\x85\xbd\xd7JZ\xe9\xb6\xcd>\x16\xce\xac\x18\xdb\xc4\x8d\xa3i\xb2\x0bMi4\xaa"\xb3\xd0\xa31\x9eN#\x8dF/T\x18\x8d\n\xe6\x86\xd1\x18\xb8\x19\xf1\xfdn\'\xcd\xa7w\xbb\xc7\xfb\xf6\xce}f\xa3y\x10q\xaf0A\x053R\x19\x93\x0f\xa6\xc9\x85T\x00\xc5\xac{m\xed\xe8d\xd1\xd4\xad\xbc\x9cir\xa1\xa6\x96\x8d\xd3\xa9b$\x19\x8a\x1d\x85\x9d\t\xa9\xa7\xf0n\xc7v\x98\x8a\x02\x05"\xcc\xc2xb(\xd0\x18vC\xe7\xc6\x10\xc6k\xba:<\xdaI\xdb\xef\x8a\x86\xc8\xf6\x10\x86HL\xaa\xe5S/H\xfa\x8c\x10J\x86(\x91G\x0f:\x8d\xdb%\x8d\x83f)\x1cH\xd2q\xe2V\x14\x98\xcd\xaa\x0c\x9e\x82]\x80\x9d\xea\x06|\xa5\x8f\xe0\x19}$N\xd8k\xa4\xe8\xd3{\xb2y\xfd\xe9`ZT\x1a)\xe6\n\xb4+\x9f\x91\xf0T\xe34\xb4JL2}\xb4\x06xY4);^\x83\xe1o\x1c\xb4\x96\x98\x00\xab\xa6td\xbcI\x96\x10\xad\x18\x14Hd.\x0fF\t\xfb0c\x81\xd1\xd3#\x80\xc13\x17\xde\xee\x89\x06\xf452&\xfd\xc15\xc2`V\xfa\xc2\xdbU\xf6\x15\xf4\x13J\xb6\x1d\x93<~(\xb5^I\xc2 \xd3\xee!\x12=\x89\x96s\xd9\xe5D\xef\x16\xfe\n\x867B\xff\xbc\xe7\t\x1eV\xc0\x040\xce\x16\x91N\xa5\x85nw\x0fU\xa9\x89\xe44\xe6\xa0\xd1V1q{\x8d\xf98\xca\x96\x9c|\xb1\xce\xe5\x9a\x86\x19\x85\xa1L\xed\x94\x8c,\xf9\x8b\x06.\xa0t\xe8b\x81\xe3\xd8N\x08\x86\xeccW\x88w\xdcT\xc1\xc47\x08\xe4\xaco\xfb\xbe\x8fg\x80\xb3\xf5\xf6\x0c\xb816\x10\xc7\x8bN\x13\x1e\x0e=\x11^\'\xa4\xa5\x02n\x033\x8a\x1b\x08\xbc\xe0;">\xc4\x04\x84\\\x1c\xc0i\xc7\x1e\x81\x909\x03\x86q\x8d\x00L\x17\x89\x04q\xba\x7f\xa8\xa8%\xf1 n?\x98\xef\xc0|t\xd1\x9e\x1d\xcdE{f{(#G\x8b\x90\xa0\x81\x1c\xc6\x96\x8f=\x90\x015,2\xb8e\xee\xa2\t\xb4\x12\xf5\xe8pH\x88\x87G\xca\xae\x95\x83\x82\tf\x84\xbf\x1c\xee\xb2}O\xd3\x848b\x84\x1dE\xf3\xe3\x14[\xec{\xd9\xce\xce\xe9\xcd\xfb\x1e.\xe7\x99A]\xec\xbb\x98o\xb2zs9v\xad\xc0\xfa\xe1\x15\xf4\xee\x18C\x88\xc5\x01\xb5g\xe7;B0y\xa93\xcb\xa0`\xa3\x1d\x83J\xbc\xcb\x8b\xd5$S\x16\xe9\xc6\x12U\xd7v\xdc\x80\xf7\xd4\xea+\x82\xec%,\x13\xa9\xceL\xd0W\x84#\x1bf\xe89\xa5uH\x8c#C3S\x86f\xda;\xa2\x14\x10\xc5#\x91\x82%\x0c\xa2\x9d\xf7\x9aV\xf0\x89iV\xe1\x9c\xf8\x08\x14r\xd7\xe9'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\SelfDrivingCar.ipynb
b'{"cells":[{"cell_type":"markdown","metadata":{},"source":["We will now import all the libraries needed for training process. It will use Tensorflow backend and keras at frontend."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\\n"]}],"source":["import os\\n","import numpy as np\\n","import matplotlib.pyplot as plt\\n","import matplotlib.image as mpimg\\n","import keras\\n","from tensorflow.keras.models import Sequential\\n","from tensorflow.keras.optimizers import Adam\\n","from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\\n","from sklearn.utils import shuffle\\n","from sklearn.model_selection import train_test_split\\n","from imgaug import augmenters as iaa\\n","import cv2\\n","import pandas as pd\\n","import ntpath\\n","import random\\n","import warnings\\n","warnings.filterwarnings(\\"ignore\\")"]},{"cell_type":"markdown","metadata":{},"source":["We wil use datadir as the name given to the fo'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\SelfDrivingCar.py
b'import os\nfrom posixpath import splitext\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport keras\nfrom numpy.core.fromnumeric import size\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom imgaug import augmenters as iaa\nimport cv2\nimport pandas as pd\nimport ntpath\nimport random\nimport warnings\n\nwarnings.filterwarnings("ignore")\n\n\ndir = "/Users/asik/Desktop/SelfDrivingCar"\ncolumns = ["center", "left", "right", "steering", "throttle", "reverse", "speed"]\ndata = pd.read_csv(os.path.join(dir, "driving_log.csv"), names=columns)\npd.set_option("display.max_colwidth", -1)\ndata.head()\n\n\ndef pathleaf(path):\n    head, tail = ntpath.split(path)\n    return tail\n\n\ndata["center"] = data["center"].apply(pathleaf)\ndata["left"] = data["left"].apply(pathleaf)\ndata["righ'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\IMG\center_2021_12_19_18_46_10_430.jpg
b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00\x00\x01\x00\x01\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.\' ",#\x1c\x1c(7),01444\x1f\'9=82<.342\xff\xdb\x00C\x01\t\t\t\x0c\x0b\x0c\x18\r\r\x182!\x1c!22222222222222222222222222222222222222222222222222\xff\xc0\x00\x11\x08\x00\xa0\x01@\x03\x01"\x00\x02\x11\x01\x03\x11\x01\xff\xc4\x00\x1f\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x10\x00\x02\x01\x03\x03\x02\x04\x03\x05\x05\x04\x04\x00\x00\x01}\x01\x02\x03\x00\x04\x11\x05\x12!1A\x06\x13Qa\x07"q\x142\x81\x91\xa1\x08#B\xb1\xc1\x15R\xd1\xf0$3br\x82\t\n\x16\x17\x18\x19\x1a%&\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe1\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf1\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xc4\x00\x1f\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x11\x00\x02\x01\x02\x04\x04\x03\x04\x07\x05\x04\x04\x00\x01\x02w\x00\x01\x02\x03\x11\x04\x05!1\x06\x12AQ\x07aq\x13"2\x81\x08\x14B\x91\xa1\xb1\xc1\t#3R\xf0\x15br\xd1\n\x16$4\xe1%\xf1\x17\x18\x19\x1a&\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x82\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xda\x00\x0c\x03\x01\x00\x02\x11\x03\x11\x00?\x00\xbeR\x98R\xad\x98\xe9\x86:\xf5\x8f<\xaaR\x9aR\xad\x14\xa6\x94\xa0eR\x94\xd2\x95h\xa54\xa5\x00U)M)V\x8aSJR\x02\xa9JB\x95d\xa54\xa5\x00V\xdb\\\x86\xa6\xd0\xdf\x13qg)\x98\xac\xea6\x06\x0c\xe5HS\x95\xce\x18\x0c\xb1\x05y\xe7\xb0\xae\xd6H\xd8\xa3\x04!_\x07ia\x90\x0f\xb8\xe35\xc1\xbcQ\xc5\xac\xcd\x00\x92\x03;\xb0ktvW*\xc4\xeddv\x04~\xf3\x80~b\xc0\xf3\xf7\x89\xc9\xe6\xc4\xdd\xc6\xc6\xb4\xb7\xb9\xd4i\xe6L\x88\x99\n\xc2T\xbc|\x1f\x94\xf5\xc1\xe3\xfc\xe2\xaem\xac\x98\xaf\x8d\xbd\xb41\x87T8\x03\x90H9\x19\xc8\xcf8\x03\x1f\x9f\x07\x8a\xdb\t\xc0\xc1$z\x9e\xf5\x86\x06\xa2i\xc3\xfe\x18\xd2\xbc\x1a\xf7\x88\xb6\xd1\xb6\xa5\xd9F\xda\xf4\x0er-\xb4\xd6V\x08\xc5\x00-\x8e\x018\x04\xfdjb\x02\xa9f \x002I=)\xad\x96B#u\x0e\xcav\x1223\xeb\x8c\x8c\x8f\xc6\x93i\x01\xc5?\xda\x1bV\x99\xe6\xb5T\x90\xa9d\xf3 1d\xa0%\x8a\xe0\x9c\x9f\xc4\x83\x90}1\xd8[4\xb2\xdb\xc7$\xc4\x17e\x19\xc1\xc8\xf4\xeb^}5\xf5\xad\xce\xac\xb7\x93\xe9\xf1\xda\xbc\x8e\xcc\xad71\xb0\xf2\xb3\x82\x0e2O\xcax\xc1\xf9\xfb\x9cWka\xaa\xe9~Z[\xdb\xccQ2B\x89\t\xc6}\x89\xeb\xdb\x9c\xfa\x0e\xbcW\x056\xa3[\x9a\xfb\x9d\x12M\xc2\xc6\x96\xda]\xb5&\xc2\x0e\x08\xe6\x94-z\'0\xc0\xb5SP\xbb{K\x07\xb8\x8a\x16\x90\xa9\xc1\x1b~\xe8\x07\x92G\x07\x1fO\xe5\xcdY\xbc\x95'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\IMG\left_2021_12_19_18_46_10_430.jpg
b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00\x00\x01\x00\x01\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.\' ",#\x1c\x1c(7),01444\x1f\'9=82<.342\xff\xdb\x00C\x01\t\t\t\x0c\x0b\x0c\x18\r\r\x182!\x1c!22222222222222222222222222222222222222222222222222\xff\xc0\x00\x11\x08\x00\xa0\x01@\x03\x01"\x00\x02\x11\x01\x03\x11\x01\xff\xc4\x00\x1f\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x10\x00\x02\x01\x03\x03\x02\x04\x03\x05\x05\x04\x04\x00\x00\x01}\x01\x02\x03\x00\x04\x11\x05\x12!1A\x06\x13Qa\x07"q\x142\x81\x91\xa1\x08#B\xb1\xc1\x15R\xd1\xf0$3br\x82\t\n\x16\x17\x18\x19\x1a%&\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe1\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf1\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xc4\x00\x1f\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x11\x00\x02\x01\x02\x04\x04\x03\x04\x07\x05\x04\x04\x00\x01\x02w\x00\x01\x02\x03\x11\x04\x05!1\x06\x12AQ\x07aq\x13"2\x81\x08\x14B\x91\xa1\xb1\xc1\t#3R\xf0\x15br\xd1\n\x16$4\xe1%\xf1\x17\x18\x19\x1a&\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x82\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xda\x00\x0c\x03\x01\x00\x02\x11\x03\x11\x00?\x00\xbeR\x98R\xad\x98\xe9\x86:\xf5\x8f<\xaaR\x9aR\xad\x14\xa6\x94\xa0eR\x94\xd2\x95h\xa54\xa5\x00U)M)V\x8aSJR\x02\xa9JB\x95d\xa54\xa5\x00V\xdb\\\xc7\x88\x1e\xca\xe5ge\xbbC%\xb6\x12Tc\xc2\x8eNW\xe5?6q\x9cv\x078\xc5u\xe5+\xcf\xfcG\x1f\xd8uX\xe2[\xa8\x19&Vi\x13#tr\x9c\x9e\xec\n\xa1-\xb8\x02\xd8\x04\x93\xd3\x83\x86#\xe04\xa7\xf1\x1b\x9a \xba\x86\xd6\x18\xda&H\xdc\x8d\xea\xca\xeaCc\xef`\x8f\\\xf7\xe3\xa7\x15\xb5\xb6\xb0t\xeb\xa3kj\x89\xe7\xb2gq`\xe8\xc0d1R\xbc\xe4\x820y\xfa\xfe\x1b\xf6\xe4\xcb\x02HpC\x0c\x86\x03\x00\xfd9?O\xc2\xb9pU,\xdd7\xeak^?hM\xb4m\xa9vQ\xb6\xbd\x13\x9c\x8bm\x1bj]\xb4\x99_\xef\x0e\xb8\xeb\xdf\xd2\x86\xd2\xdc\x0eSP\xbd\x8a\x0f\x10\x05\x929 \x8d\x82\xc71\x9a \xd1K\x9c\xed\'\x9c\x0e\x9c\x12{\x1e\x9bH;\x9aL\x8d-\x84~cD\xce\xa3\x19\x8c\xf0Gc\xff\x00\xeb\xe6\xb9\xcb\xf8\xcc\xda\x9f\xd9\xa3\xb6\x9eK\xff\x002R\xb2*2mR\xcc\x01f+\xf3!\x0c\x17\xd0l88 \xd6\xce\x8fyi\n\x0bv\x90\xfd\xa5\xb1\xe7aH]\xf9\xc61\xd8\xe7\xb6\x07_N\x9c\x17J\xba\x937\xb5\xe1dlm\xa5\xdbOP\x18\x02\x08 \xf2\x08\xa7\x05\xafA\x18\x0c\x0bQ\xc8\xdbO\xfa\xb7g_\x99QX\x02\xe3\x00\x129\x19\x1c\xf7\xff\x00\n\x99\xb6\x90\xc8C\xf2\x00;A\xef\xc7\x04\x7f\x91XM\xa8'
Data\aslanahmedov\self-driving-carbehavioural-cloning\self-driving-carbehavioural-cloning\IMG\right_2021_12_19_18_46_10_430.jpg
b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00\x00\x01\x00\x01\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.\' ",#\x1c\x1c(7),01444\x1f\'9=82<.342\xff\xdb\x00C\x01\t\t\t\x0c\x0b\x0c\x18\r\r\x182!\x1c!22222222222222222222222222222222222222222222222222\xff\xc0\x00\x11\x08\x00\xa0\x01@\x03\x01"\x00\x02\x11\x01\x03\x11\x01\xff\xc4\x00\x1f\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x10\x00\x02\x01\x03\x03\x02\x04\x03\x05\x05\x04\x04\x00\x00\x01}\x01\x02\x03\x00\x04\x11\x05\x12!1A\x06\x13Qa\x07"q\x142\x81\x91\xa1\x08#B\xb1\xc1\x15R\xd1\xf0$3br\x82\t\n\x16\x17\x18\x19\x1a%&\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe1\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf1\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xc4\x00\x1f\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x11\x00\x02\x01\x02\x04\x04\x03\x04\x07\x05\x04\x04\x00\x01\x02w\x00\x01\x02\x03\x11\x04\x05!1\x06\x12AQ\x07aq\x13"2\x81\x08\x14B\x91\xa1\xb1\xc1\t#3R\xf0\x15br\xd1\n\x16$4\xe1%\xf1\x17\x18\x19\x1a&\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x82\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xda\x00\x0c\x03\x01\x00\x02\x11\x03\x11\x00?\x00\xbeR\x98R\xad\x98\xe9\x86:\xf5\x8f<\xaaR\x9aR\xad\x14\xa6\x94\xa0eR\x94\xd2\x95h\xa54\xa5\x00U)M)V\x8aSJR\x02\xa9JB\x95d\xa54\xa5\x00U|"\x96r\x15Td\x92p\x00\xae\x16\xf26\xb9\xd4b\x96\xdeF?:\x94Q\x97vB\xdc31P1\x8cu$\x8e3\xd3\x15\xde]\xac\x82\x03\xb27\x90td\x8c\x80\xec\x0f\x1f),0z\x1c\xe7\xb7\xady\xf5\x83yWmm,\x96\xedte\t\x1c\xb0\xb6\xe8X\xbb(\x00\x04\xe1z\x1e\xdf\xc4z\x15\x00\xf1\xe2\xee\xe2\x927\xa3k\xdc\xed\xad\x1af\x8f\xcb\x9a=\x8d\x18\n\x08$\x821\xd0\x12\x07J\xb1\xb6\xb3\xac\xef\xday-\xd0\xce\x8a$\xc6\x14\xaf g\x1dI\xe4\xfdO\xe1\xd0\xd6\xbe\xca0Uc(r.\x81Z\r;\xbe\xa4;h\xdbR\xec\xa3mv\x18\x91m\xa8nH\x8e\xd6F0<\xea\x17\x98\xd1A,;\x8c\x1e\xbfJ\xb4\xdbQw1\n=I\xc5U\xd4\x9eH\xac\xd9\xa3\x81g\x19\x02H\xca\x96%\t\xe4\x05\x00\xe4\x91\xc0\xce\x07rp)6\x92\x04r\xfa<\xa4"O\x04ee\x12\x8f\x96\xe0\xfc\xdb\n\xaf\x1b\x8e\x01\x03\x0b\xdb\x80\xc3\xd6\xbb\r\xb5\xc1\xe9\x87\xed\x12\xad\xd3\xdb\xc7\x06\x9a\x8e\x04\xff\x00+\x05\xde@\xcb)\xfe\x1c\xe0}\xd0\x07A\xf5\xedm/`\x9f\xca\x897\x07`\x02&w\x900q\x92\xb9\x03\x85=\xfbW\x06\x1eQ\x8dGw\xb9\xbdD\xdcU\x8b\x1biv\xd4\x81iB\xd7\xa2s\x8c\x0bT\xef\xee\xa5\xb5\x08\xa2\'>c\x85\x0e\x838\xc9Q\xdf'

Use DATASET METADATA and DATASET FILES CONTENTS to compose a dataset summary that has ONLY the following sections:
1) "What this data is about": a single paragraph that describes the data in general without details
2) "How this data could be used": a short list of the potential uses of the data in general without details

