{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aarzookuhar hotel-recommendation-dataset\n",
      "1 abdelrahman16 car-data\n",
      "2 abdelrhamanfakhry movies-data-for-ml-dl-recommendation-system\n",
      "3 abderrahimalakouche flight-delay-prediction\n",
      "4 abdullahkhanuet22 olx-cars-dataset\n",
      "5 abdullahorzan moodify-dataset\n",
      "6 abhishtagatya my-anime-list-2021\n",
      "7 abisheksudarshan topic-modeling-for-research-articles\n",
      "8 abrambeyer us-hospital-overall-star-ratings-20162020\n",
      "9 abuchionwuegbusi movie-recommendation\n",
      "10 adhamelkomy news-classification-and-analysis-using-nlp\n",
      "11 adinishad yts-movie-dataset\n",
      "12 adithyaawati apartments-for-rent-classified\n",
      "13 adityadesai13 used-car-dataset-ford-and-mercedes\n",
      "14 adityadeshpande23 amsterdam-airbnb\n",
      "15 adityaramachandran27 nasa-near-earth-objects-information\n",
      "16 aguado nyc-taxi-jan-aug-2022\n",
      "17 agungpambudi africa-soil-mapping-isdasoil-exploration\n",
      "18 ahmedaliraja customer-rating-data-by-amazon\n",
      "19 ahmedashrafahmed household-power-consumption\n",
      "20 ahmedshahriarsakib usa-real-estate-dataset\n",
      "21 ahmettezcantekin beginner-datasets\n",
      "22 ahsanaseer top-rated-tmdb-movies-10k\n",
      "23 ajaysh women-apparel-recommendation-engine-amazoncom\n",
      "24 akash14 house-price-dataset\n",
      "25 akhilups insurance-product-purchase-prediction\n",
      "26 akouaorsot empiricisms-thinkers\n",
      "27 akshayamali netflix-clustering-and-recommendation\n",
      "28 aleexharris bitcoin-network-on-chain-blockchain-data\n",
      "29 aleksandrglotov car-prices-poland\n",
      "30 alessiasimone lego-sets-and-price-1955-2023\n",
      "31 alexanderfrosati goodbooks-10k-updated\n",
      "32 allegray book-recommendation-system-dataset\n",
      "33 amanbarthwal imdb-movies-data\n",
      "34 amanverma1999 a-comprehensive-dataset-for-ddos-attack\n",
      "35 amariaziz tunisian-stock-market\n",
      "36 amirhoseinsedaghati the-weather-of-187-countries-in-2020\n",
      "37 amiroft tehran-renting\n",
      "38 amolbhone lead-score-case-study\n",
      "39 anandaramg apartment-cost-in-new-york-city\n",
      "40 anandpuntambekar loan-case-study\n",
      "41 anashamoutni optical-care-reimbursements-in-france-20092021\n",
      "42 andreagarritano wikipedia-article-networks\n",
      "43 aniketsharma00411 wikihow-features\n",
      "44 anmolkumar house-price-prediction-challenge\n",
      "45 annecool37 museum-data\n",
      "46 anthonytherrien 20000-coding-questions-solved-with-code-llama-70b\n",
      "47 anutural product-review-dataset\n",
      "48 apkaayush tmdb-10000-movies-dataset\n",
      "49 arashnic an-unbiased-sequential-recommendation-dataset\n",
      "50 arbazkhan971 the-great-indian-hiring-hackathon\n",
      "51 arianghasemi iran-phone-ads\n",
      "52 arindamsahoo social-media-users\n",
      "53 arkaradeniz linear-regression-example-dataset\n",
      "54 arpikr uci-drug\n",
      "55 arplusman papers-by-subject\n",
      "56 arunklenin ps4e4-ensemble-ancillary\n",
      "57 aryansakhala coffee-recommendation\n",
      "58 asaniczka 52000-animation-movie-details-dataset-2024\n",
      "59 asharalikamil regression-technique-eda\n",
      "60 ashishkumarsingh123 telecom-churn-dataset\n",
      "61 ashishvaya recommendation-engine\n",
      "62 aslanahmedov self-driving-carbehavioural-cloning\n",
      "63 astronautelvis anime-recommendation\n",
      "64 austcse embedded-smartphone-sensor-data\n",
      "65 BidecInnovations stock-price-and-news-realted-to-it\n",
      "66 CooperUnion anime-recommendations-database\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "\n",
    "data_dir_name = \"Data\"\n",
    "\n",
    "for i, user_name in enumerate(os.listdir(data_dir_name)):\n",
    "    \n",
    "    dataset_name = os.listdir(os.path.join(data_dir_name, user_name))[0]\n",
    "    path = os.path.join(data_dir_name, user_name, dataset_name)\n",
    "\n",
    "    print(i, user_name, dataset_name)\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        \n",
    "        if filename.endswith('.json'):\n",
    "            \n",
    "            # metadata_prompt = \"DATASET METADATA:\\n\\n\"\n",
    "\n",
    "            # with open(os.path.join(path, filename)) as f:\n",
    "            #     metadata_prompt += f\"{f.read()}\\n\\n\"\n",
    "\n",
    "            metadata_prompt = \"\"\n",
    "\n",
    "            with open(os.path.join(path, filename)) as f:\n",
    "                metadata_json = json.load(f)\n",
    "\n",
    "            if 'id' in metadata_json and metadata_json['id'] != '':\n",
    "                metadata_prompt += f\"ID ДАТАСЕТА:\\n{metadata_json['id'][:1024]}\\n\\n\"\n",
    "\n",
    "            if 'hasTitle' in metadata_json and metadata_json['hasTitle'] == True:\n",
    "                if 'title' in metadata_json and metadata_json['title'] != '':\n",
    "                    metadata_prompt += f\"ЗАГОЛОВОК ДАТАСЕТА:\\n{metadata_json['title'][:1024]}\\n\\n\"\n",
    "\n",
    "            if 'hasSubtitle' in metadata_json and metadata_json['hasSubtitle'] == True:\n",
    "                if 'subtitle' in metadata_json and metadata_json['subtitle'] != '':\n",
    "                    metadata_prompt += f\"ПОДЗАГОЛОВОК ДАТАСЕТА:\\n{metadata_json['subtitle'][:1024]}\\n\\n\"\n",
    "\n",
    "            if 'hasDescription' in metadata_json and metadata_json['hasDescription'] == True:\n",
    "                if 'description' in metadata_json and metadata_json['description'] != '':\n",
    "                    metadata_prompt += f\"ОПИСАНИЕ ДАТАСЕТА:\\n{metadata_json['subtitle'][:1024]}\\n\\n\"\n",
    "\n",
    "            if 'keywords' in metadata_json and len(metadata_json['keywords']) != 0:\n",
    "                metadata_prompt += f\"КЛЮЧЕВЫЕ СЛОВА ДАТАСЕТА:\\n{str(metadata_json['keywords'])[:1024]}\\n\\n\"\n",
    "\n",
    "\n",
    "        if filename.endswith('.zip'):\n",
    "            \n",
    "            data_prompt = \"СОДЕРЖИМОЕ ФАЙЛОВ ДАТАСЕТА\"\n",
    "\n",
    "            extraction_dir_name = os.path.join(path, filename[:-4])\n",
    "            \n",
    "            if not os.path.exists(extraction_dir_name):\n",
    "                os.makedirs(extraction_dir_name)\n",
    "                with zipfile.ZipFile(os.path.join(path, filename), 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extraction_dir_name)\n",
    "            \n",
    "            all_paths = glob.glob(extraction_dir_name + \"/**/*\", recursive=True)\n",
    "            file_paths = [path for path in all_paths if os.path.isfile(path)]\n",
    "\n",
    "            if len(file_paths) > 4:\n",
    "                data_prompt += f\" (показано только 4 файла из {len(file_paths)})\"\n",
    "            \n",
    "            data_prompt += \":\\n\"\n",
    "\n",
    "            # max number of files is 4\n",
    "            for data_filename in file_paths[:4]:\n",
    "\n",
    "                data_prompt += f\"Имя файла: {data_filename}\\n\"\n",
    "\n",
    "                with open(data_filename, 'rb') as f:\n",
    "                    data_file_content = f.read(1024)\n",
    "\n",
    "                data_prompt += f\"Первые 1024 байт файла: {data_file_content}\\n\\n\"\n",
    "\n",
    "    prompt = metadata_prompt + data_prompt + \"\\n\"\n",
    "\n",
    "    # # Collection method prompt\n",
    "    # prompt += \"Используй информацию из секций, обозначенных ЗАГЛАВНЫМИ буквами, чтобы составить один абзац текста описывающий \"\n",
    "    # prompt += \"как данные из датасета были собраны. Если в секциях выше нет информации о том, как данные были собраны, то просто сообщи об этом. \"\n",
    "    # prompt += \"Свой ответ составь на русском языке.\"\n",
    "\n",
    "    # # Dataset structure prompt\n",
    "    # prompt += \"Используй информацию из секций, обозначенных ЗАГЛАВНЫМИ буквами, чтобы составить один абзац текста описывающий \"\n",
    "    # prompt += \"структуру датасета. Если в секциях выше недостаточно информации о структуре датасета, то просто сообщи об этом. \"\n",
    "    # prompt += \"Свой ответ составь на русском языке.\"\n",
    "\n",
    "    # Dataset usecases prompt\n",
    "    prompt += \"Используй информацию из секций, обозначенных ЗАГЛАВНЫМИ буквами, чтобы составить один абзац текста описывающий \"\n",
    "    prompt += \"возможные варианты использования датасета. \"\n",
    "    prompt += \"Свой ответ составь на русском языке.\"\n",
    "\n",
    "    prompt_filename = dataset_name.replace(\"\\\\\", \"_\") + \".txt\" \n",
    "    # with open(os.path.join('CollectionMethodPrompts', prompt_filename), 'w') as f:\n",
    "    # with open(os.path.join('DatasetStructurePrompts', prompt_filename), 'w') as f:\n",
    "    with open(os.path.join('PotentialUsesPrompts', prompt_filename), 'w') as f:\n",
    "        f.write(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data\\\\adithyaawati\\\\apartments-for-rent-classified\\\\apartments-for-rent-classified\\\\apartments_for_rent_classified_100K\\\\apartments_for_rent_classified_100K.csv', 'Data\\\\adithyaawati\\\\apartments-for-rent-classified\\\\apartments-for-rent-classified\\\\apartments_for_rent_classified_10K\\\\apartments_for_rent_classified_10K.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "directory = os.path.join(\"Data\", \"adithyaawati\", \"apartments-for-rent-classified\", \"apartments-for-rent-classified\")\n",
    "\n",
    "all_paths = glob.glob(directory + \"/**/*\", recursive=True)\n",
    "\n",
    "file_paths = [path for path in all_paths if os.path.isfile(path)]\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('datasets_names_list.txt', 'w') as f:\n",
    "    for user_name in os.listdir('Data'):\n",
    "        dataset_name = os.listdir(os.path.join('Data', user_name))[0]\n",
    "        f.write(f\"{user_name}/{dataset_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
